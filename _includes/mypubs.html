<!-- Item: DennisAVSSJ19 -->
<li >
<b>ShaRNN: A Method for Accurate Time-series Classification on Tiny Devices</b><br>
Don Dennis, Alp Acar, Mandikal Vikram, Harsha Simhadri, Venkatesh Saligrama and Prateek Jain,<br>
in <i>Proceedings of the Thirty-second Annual Conference on Neural Information Processing Systems (NeurIPS)</i>,
2019.
<br />
<a href="javascript:toggleBibtex('DennisAVSSJ19')">[BibTeX]</a>
<a id="displayTextDennisAVSSJ19" href="javascript:toggle('DennisAVSSJ19');">[Abstract]</a>
<a href="all_papers/DennisAVSSJ19.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="posters/DennisAVSSJ19.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/DennisAVSSJ19.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextDennisAVSSJ19" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">.</div>
<div id="bib_DennisAVSSJ19" class="bibtex noshow">
<pre>
@inproceedings{DennisAVSSJ19,
  author = {Don Dennis and Alp Acar and Mandikal Vikram and Harsha Simhadri and Venkatesh Saligrama and Prateek Jain },
  title = {ShaRNN: A Method for Accurate Time-series Classification on Tiny Devices},
  booktitle = {Proceedings of the Thirty-second Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2019},
  url = {all_papers/DennisAVSSJ19.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: TJNO19 -->
<li >
<b>Efficient Algorithms for Smooth Minimax Optimization</b><br>
Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli and Sewoong Oh,<br>
in <i>Proceedings of the Thirty-second Annual Conference on Neural Information Processing Systems (NeurIPS)</i>,
2019.
<br />
<a href="javascript:toggleBibtex('TJNO19')">[BibTeX]</a>
<a id="displayTextTJNO19" href="javascript:toggle('TJNO19');">[Abstract]</a>
<a href="all_papers/TJNO19.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="posters/TJNO19.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/TJNO19.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextTJNO19" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">This paper studies first order methods for solving smooth minimax optimization problems  where  is smooth and  is concave for each . In terms of , we consider two settings -- strongly convex and nonconvex -- and improve upon the best known rates in both. For strongly-convex , we propose a new algorithm combining Mirror-Prox and Nesterov's AGD, and show that it can find global optimum in  iterations, improving over current state-of-the-art rate of . We use this result along with an inexact proximal point method to provide  rate for finding stationary points in the nonconvex setting where  can be nonconvex. This improves over current best-known rate of . Finally, we instantiate our result for finite nonconvex minimax problems, i.e., , with nonconvex , to obtain convergence rate of  total gradient evaluations for finding a stationary point.</div>
<div id="bib_TJNO19" class="bibtex noshow">
<pre>
@inproceedings{TJNO19,
  author = {Kiran Koshy Thekumparampil and Prateek Jain and Praneeth Netrapalli and Sewoong Oh},
  title = {Efficient Algorithms for Smooth Minimax Optimization},
  booktitle = {Proceedings of the Thirty-second Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2019},
  url = {all_papers/TJNO19.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: ZhongSJD19 -->
<li >
<b>Nonlinear Inductive Matrix Completion based on One-layer Neural Networks</b><br>
Kai Zhong, Zhao Song, Prateek Jain and Inderjit S. Dhillon,<br>
in <i>Proceedings of the Thirty-second Annual Conference on Neural Information Processing Systems (NeurIPS)</i>,
2019.
<br />
<a href="javascript:toggleBibtex('ZhongSJD19')">[BibTeX]</a>
<a id="displayTextZhongSJD19" href="javascript:toggle('ZhongSJD19');">[Abstract]</a>
<a href="all_papers/ZhongSJD19.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="posters/TJNO19.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/TJNO19.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextZhongSJD19" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">The goal of a recommendation system is to predict the interest of a user in a given item by exploiting the existing set of ratings as well as certain user/item features. A standard approach to modeling this problem is Inductive Matrix Completion where the predicted rating is modeled as an inner product of the user and the item features projected onto a latent space. In order to learn the parameters effectively from a small number of observed ratings, the latent space is constrained to be low-dimensional which implies that the parameter matrix is constrained to be low-rank. However, such bilinear modeling of the ratings can be limiting in practice and non-linear prediction functions can lead to significant improvements. A natural approach to introducing non-linearity in the prediction function is to apply a non-linear activation function on top of the projected user/item features. Imposition of non-linearities further complicates an already challenging problem that has two sources of non-convexity: a) low-rank structure of the parameter matrix, and b) non-linear activation function. We show that one can still solve the non-linear Inductive Matrix Completion problem using gradient descent type methods as long as the solution is initialized well. That is, close to the optima, the optimization function is strongly convex and hence admits standard optimization techniques, at least for certain activation functions, such as Sigmoid and tanh. We also highlight the importance of the activation function and show how ReLU can behave significantly differently than say a sigmoid function. Finally, we apply our proposed technique to recommendation systems and semi-supervised clustering, and show that our method can lead to much better performance than standard linear Inductive Matrix Completion methods.</div>
<div id="bib_ZhongSJD19" class="bibtex noshow">
<pre>
@inproceedings{ZhongSJD19,
  author = {Kai Zhong and Zhao Song and Prateek Jain and Inderjit S. Dhillon},
  title = {Nonlinear Inductive Matrix Completion based on One-layer Neural Networks},
  booktitle = {Proceedings of the Thirty-second Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2019},
  url = {all_papers/ZhongSJD19.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: PatilDPSSSVJ19 -->
<li >
<b>GesturePod: Enabling On-device Gesture-based Interaction for White Cane Users</b><br>
Shishir G. Patil, Don Kurian Dennis, Chirag Pabbaraju, Nadeem Shaheer, Harsha Vardhan Simhadri, Vivek Seshadri, Manik Varma and Prateek Jain,<br>
in <i>Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST)</i>,
2019.
<br />
<a href="javascript:toggleBibtex('PatilDPSSSVJ19')">[BibTeX]</a>
<a id="displayTextPatilDPSSSVJ19" href="javascript:toggle('PatilDPSSSVJ19');">[Abstract]</a>
<a href="all_papers/PatilDPSSSVJ19.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/PatilDPSSSVJ19.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/PatilDPSSSVJ19.pdf>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextPatilDPSSSVJ19" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">People using white canes for navigation find it challenging to concurrently access devices such as smartphones. Building on prior research on abandonment of specialized devices, we explore a new touch free mode of interaction wherein a person with visual impairment can perform gestures on their existing white cane to trigger tasks on their smartphone. We present GesturePod, an easy-to-integrate device that clips on to any white cane, and detects gestures performed with the cane. With GesturePod, a user can perform common tasks on their smartphone without touch or even removing the phone from their pocket or bag. We discuss the challenges in building the device and our design choices. We propose a novel, efficient machine learning pipeline to train and deploy the gesture recognition model. Our in-lab study shows that GesturePod achieves 92 percent gesture recognition accuracy and can help perform common smartphone tasks faster. Our in-wild study suggests that GesturePod is a promising tool to improve smartphone access for people with VI, especially in constrained outdoor scenarios.</div>
<div id="bib_PatilDPSSSVJ19" class="bibtex noshow">
<pre>
@inproceedings{PatilDPSSSVJ19,
  author = {Shishir G. Patil and Don Kurian Dennis and Chirag Pabbaraju and Nadeem Shaheer and Harsha Vardhan Simhadri and Vivek Seshadri and Manik Varma and Prateek Jain},
  title = {GesturePod: Enabling On-device Gesture-based Interaction for White Cane Users},
  booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST)},
  year = {2019},
  pages = {403--415},
  note = {slides/PatilDPSSSVJ19.pdf},
  url = {all_papers/PatilDPSSSVJ19.pdf},
  doi = {https://doi.org/10.1145/3332165.3347881}
}
</pre>
</div>
</li>
<br /><!-- Item: JainNN19 -->
<li >
<b>Making the Last Iterate of SGD Information Theoretically Optimal</b><br>
Prateek Jain, Dheeraj Nagaraj and Praneeth Netrapalli,<br>
in <i>Proceedings of the Annual Conference On Learning Theory (COLT)</i>,
2019.
<br />
<a href="javascript:toggleBibtex('JainNN19')">[BibTeX]</a>
<a id="displayTextJainNN19" href="javascript:toggle('JainNN19');">[Abstract]</a>
<a href="all_papers/JainNN19">[URL]</a>

<script type="text/javascript">
var slide="slides/JainNN19.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/JainNN19.pdf>[Slides]</a>");
}
var poster="posters/JainNN19.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/JainNN19.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainNN19" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Stochastic gradient descent (SGD) is one of the most widely used algorithms for large scale optimization problems. While classical theoretical analysis of SGD for convex problems studies (suffix) {averages} of iterates and obtains information theoretically optimal bounds on suboptimality, the {last point} of SGD is, by far, the most preferred choice in practice. The best known results for last point of SGD (Shamir and Zhang, 2013) however, are suboptimal compared to information theoretic lower bounds by a logT factor, where T is the number of iterations. Harvey et. al (2018) shows that in fact, this additional logT factor is tight for standard step size sequences of 1/sqrt(t) and 1/t for non-strongly convex and strongly convex settings, respectively. Similarly, even for subgradient descent (GD) when applied to non-smooth, convex functions, the best known step-size sequences still lead to O(logT)-suboptimal convergence rates (on the final iterate). The main contribution of this work is to design new step size sequences that enjoy information theoretically optimal bounds on the suboptimality of {last point} of SGD as well as GD. We achieve this by designing a modification scheme, that converts one sequence of step sizes to another so that the last point of SGD/GD with modified sequence has the same suboptimality guarantees as the average of SGD/GD with original sequence. We also show that our result holds with high-probability. We validate our results through simulations which demonstrate that the new step size sequence indeed improves the final iterate significantly compared to the standard step size sequences.</div>
<div id="bib_JainNN19" class="bibtex noshow">
<pre>
@inproceedings{JainNN19,
  author = {Prateek Jain and Dheeraj Nagaraj and Praneeth Netrapalli},
  title = {Making the Last Iterate of SGD Information Theoretically Optimal},
  booktitle = {Proceedings of the Annual Conference On Learning Theory (COLT)},
  year = {2019},
  pages = {1752--1755},
  note = {slides/JainNN19.pdf},
  url = {all_papers/JainNN19}
}
</pre>
</div>
</li>
<br /><!-- Item: SuggalaBRJ19 -->
<li >
<b>Adaptive Hard Thresholding for Near-optimal Consistent Robust Regression</b><br>
Arun Sai Suggala, Kush Bhatia, Pradeep Ravikumar and Prateek Jain,<br>
in <i>Proceedings of the Annual Conference On Learning Theory (COLT)</i>,
2019.
<br />
<a href="javascript:toggleBibtex('SuggalaBRJ19')">[BibTeX]</a>
<a id="displayTextSuggalaBRJ19" href="javascript:toggle('SuggalaBRJ19');">[Abstract]</a>
<a href="all_papers/SuggalaBRJ19.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/SuggalaBRJ19.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/SuggalaBRJ19.pdf>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextSuggalaBRJ19" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We study the problem of robust linear regression with response variable corruptions. We consider the oblivious adversary model, where the adversary corrupts a fraction of the responses in complete ignorance of the data. We provide a nearly linear time estimator which consistently estimates the true regression vector, even with 1−o(1) fraction of corruptions. Existing results in this setting either don’t guarantee consistent estimates or can only handle a small fraction of corruptions. We also extend our estimator to robust sparse linear regression and show that similar guarantees hold in this setting. Finally, we apply our estimator to the problem of linear regression with heavy-tailed noise and show that our estimator consistently estimates the regression vector even when the noise has unbounded variance (e.g., Cauchy distribution), for which most existing results don’t even apply. Our estimator is based on a novel variant of outlier removal via hard thresholding in which the threshold is chosen adaptively and crucially relies on randomness to escape bad fixed points of the non-convex hard thresholding operation.</div>
<div id="bib_SuggalaBRJ19" class="bibtex noshow">
<pre>
@inproceedings{SuggalaBRJ19,
  author = {Arun Sai Suggala and Kush Bhatia and Pradeep Ravikumar and Prateek Jain},
  title = {Adaptive Hard Thresholding for Near-optimal Consistent Robust Regression},
  booktitle = {Proceedings of the Annual Conference On Learning Theory (COLT)},
  year = {2019},
  pages = {2892--2897},
  note = {slides/SuggalaBRJ19.pdf},
  url = {all_papers/SuggalaBRJ19.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: NagarajJN19 -->
<li >
<b>SGD without Replacement: Sharper Rates for General Smooth Convex Functions</b><br>
Dheeraj Nagaraj, Prateek Jain and Praneeth Netrapalli,<br>
in <i>Proceedings of the 36th International Conference on Machine Learning (ICML)</i>,
2019.
<br />
<a href="javascript:toggleBibtex('NagarajJN19')">[BibTeX]</a>
<a id="displayTextNagarajJN19" href="javascript:toggle('NagarajJN19');">[Abstract]</a>
<a href="all_papers/NagarajJN19.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/NagarajJN19.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/NagarajJN19.pdf>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextNagarajJN19" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We study stochastic gradient descent without replacement (SGDo) for smooth convex functions. SGDo is widely observed to converge faster than true SGD where each sample is drawn independently with replacement (Bottou,2009) and hence, is more popular in practice. But it’s convergence properties are not well understood as sampling without replacement leads to coupling between iterates and gradients. By using method of exchangeable pairs to bound Wasserstein distance, we provide the first non-asymptotic results for SGDo when applied to general smooth, strongly-convex functions. In particular, we show that SGDo converges at a rate of O(1/K2) while SGD is known to converge at O(1/K) rate, where K denotes the number of passes over data and is required to be large enough. Existing results for SGDo in this setting require additional Hessian Lipschitz assumption (Gurbuzbalaban et al, 2015; HaoChen and Sra 2018). For small K, we show SGDo can achieve same convergence rate as SGD for general smooth strongly-convex functions. Existing results in this setting require K=1 and hold only for generalized linear models (Shamir,2016). In addition, by careful analysis of the coupling, for both large and small K, we obtain better dependence on problem dependent parameters like condition number.</div>
<div id="bib_NagarajJN19" class="bibtex noshow">
<pre>
@inproceedings{NagarajJN19,
  author = {Dheeraj Nagaraj and Prateek Jain and Praneeth Netrapalli},
  title = {SGD without Replacement: Sharper Rates for General Smooth Convex Functions},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year = {2019},
  pages = {4703--4711},
  note = {slides/NagarajJN19.pdf},
  url = {all_papers/NagarajJN19.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: MukhotyGJK19 -->
<li >
<b>Globally-convergent Iteratively Reweighted Least Squares for Robust Regression Problems</b><br>
Bhaskar Mukhoty, Govind Gopakumar, Prateek Jain and Purushottam Kar,<br>
in <i>The 22nd International Conference on Artificial Intelligence and Statistics ( AISTATS) </i>,
2019.
<br />
<a href="javascript:toggleBibtex('MukhotyGJK19')">[BibTeX]</a>
<a id="displayTextMukhotyGJK19" href="javascript:toggle('MukhotyGJK19');">[Abstract]</a>
<a href="all_papers/MukhotyGJK19">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="posters/MukhotyGJK19.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/MukhotyGJK19.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextMukhotyGJK19" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We provide the first global model recovery results for the IRLS (iteratively reweighted least squares) heuristic for robust regression problems. IRLS is known to offer excellent performance, despite bad initializations and data corruption, for several parameter estimation problems. Existing analyses of IRLS frequently require careful initialization, thus offering only local convergence guarantees. We remedy this by proposing augmentations to the basic IRLS routine that not only offer guaranteed global recovery, but in practice also outperform state-of-the-art algorithms for robust regression. Our routines are more immune to hyperparameter misspecification in basic regression tasks, as well as applied tasks such as linear-armed bandit problems. Our theoretical analyses rely on a novel extension of the notions of strong convexity and smoothness to weighted strong convexity and smoothness, and establishing that sub-Gaussian designs offer bounded weighted condition numbers. These notions may be useful in analyzing other algorithms as well.</div>
<div id="bib_MukhotyGJK19" class="bibtex noshow">
<pre>
@inproceedings{MukhotyGJK19,
  author = {Bhaskar Mukhoty and Govind Gopakumar and Prateek Jain and Purushottam Kar},
  title = {Globally-convergent Iteratively Reweighted Least Squares for Robust Regression Problems},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics ( AISTATS) },
  year = {2019},
  pages = {313--322},
  url = {all_papers/MukhotyGJK19}
}
</pre>
</div>
</li>
<br /><!-- Item: NatarajanSDJG19 -->
<li >
<b>Learning Natural Programs from a Few Examples in Real-Time</b><br>
Nagarajan Natarajan, Danny Simmons, Naren Datha, Prateek Jain and Sumit Gulwani,<br>
in <i>Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS)</i>,
2019.
<br />
<a href="javascript:toggleBibtex('NatarajanSDJG19')">[BibTeX]</a>
<a id="displayTextNatarajanSDJG19" href="javascript:toggle('NatarajanSDJG19');">[Abstract]</a>
<a href="all_papers/NatarajanSDJG19.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/NatarajanSDJG19.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/NatarajanSDJG19.pdf>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextNatarajanSDJG19" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Programming by examples (PBE) is a rapidly growing subfield of AI, that aims to synthesize user-intended programs using input-output examples from the task. As users can provide only a few I/O examples, capturing user-intent accurately and ranking user-intended programs over other programs is challenging even in the simplest of the domains. Commercially deployed PBE systems often require years of engineering effort and domain expertise to devise ranking heuristics for real-time synthesis of accurate programs. But such heuristics may not cater to new domains, or even to a different segment of users from the same domain. In this work, we develop a novel, real-time, ML-based program ranking algorithm that enables synthesis of natural, user-intended, personalized programs. We make two key technical contributions: 1) a new technique to embed programs in a vector space making them amenable to ML-formulations, 2) a novel formulation that interleaves program search with ranking, enabling real-time synthesis of accurate user-intended programs. We implement our solution in the state-of-the-art PROSE framework. The proposed approach learns the intended program with just {\em one} I/O example in a variety of real-world string/date/number manipulation tasks, and outperforms state-of-the-art neural synthesis methods along multiple metrics.</div>
<div id="bib_NatarajanSDJG19" class="bibtex noshow">
<pre>
@inproceedings{NatarajanSDJG19,
  author = {Nagarajan Natarajan and Danny Simmons and Naren Datha and Prateek Jain and Sumit Gulwani},
  title = {Learning Natural Programs from a Few Examples in Real-Time},
  booktitle = {Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2019},
  pages = {1714--1722},
  note = {slides/NatarajanSDJG19.pdf},
  url = {all_papers/NatarajanSDJG19.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: DennisPSJ18 -->
<li >
<b>Multiple Instance Learning for Efficient Sequential Data Classification on Resource-constrained Devices</b><br>
Don Dennis, Chirag Pabbaraju, Harsha Vardhan Simhadri and Prateek Jain,<br>
in <i>Proceedings of the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)</i>,
2018.
<br />
<a href="javascript:toggleBibtex('DennisPSJ18')">[BibTeX]</a>
<a id="displayTextDennisPSJ18" href="javascript:toggle('DennisPSJ18');">[Abstract]</a>
<a href="all_papers/DennisPSJ18.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/DennisPSJ18.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/DennisPSJ18.pdf>[Slides]</a>");
}
var poster="posters/DennisPSJ18.pdf";
var code="code/emirnn.html";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/DennisPSJ18.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=code/emirnn.html>[Code]</a>");
}
</script>



<div id="toggleTextDennisPSJ18" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We study the problem of fast and efficient classification of sequential data (such as time-series) on tiny devices, which is critical for various IoT related applications like audio keyword detection or gesture detection. Such tasks are cast as a standard classification task by sliding windows over the data stream to construct data points. Deploying such classification modules on tiny devices is challenging as predictions over sliding windows of data need to be invoked continuously at a high frequency. Each such predictor instance in itself is expensive as it evaluates large models over long windows of data. In this paper, we address this challenge by exploiting the following two observations about classification tasks arising in typical IoT related applications: (a) the "signature" of a particular class (e.g. an audio keyword) typically occupies a small fraction of the overall data, and (b) class signatures tend to be discernible early on in the data. We propose a method, EMI-RNN, that exploits these observations by using a multiple instance learning formulation along with an early prediction technique to learn a model that achieves better accuracy compared to baseline models, while simultaneously reducing computation by a large fraction. For instance, on a gesture detection benchmark, EMI-RNN improves standard LSTM modelâ€™s accuracy by up to 1 percent while requiring 72x less computation. This enables us to deploy such models for continuous real-time prediction on a small device such as Raspberry Pi0 and Arduino variants, a task that the baseline LSTM could not achieve. Finally, we also provide an analysis of our multiple instance learning algorithm in a simple setting and show that the proposed algorithm converges to the global optima at a linear rate, one of the first such result in this domain. The code for EMI-RNN is available at: https://github.com/Microsoft/EdgeML/tree/master/tf/examples/EMI-RNN</div>
<div id="bib_DennisPSJ18" class="bibtex noshow">
<pre>
@inproceedings{DennisPSJ18,
  author = {Don Dennis and Chirag Pabbaraju and Harsha Vardhan Simhadri and Prateek Jain},
  title = {Multiple Instance Learning for Efficient Sequential Data Classification on Resource-constrained Devices},
  booktitle = {Proceedings of the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2018},
  pages = {10976--10987},
  note = {slides/DennisPSJ18.pdf},
  url = {all_papers/DennisPSJ18.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: KusupatiSBKJV18 -->
<li >
<b>FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network</b><br>
Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain and Manik Varma,<br>
in <i>Proceedings of the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)</i>,
2018.
<br />
<a href="javascript:toggleBibtex('KusupatiSBKJV18')">[BibTeX]</a>
<a id="displayTextKusupatiSBKJV18" href="javascript:toggle('KusupatiSBKJV18');">[Abstract]</a>
<a href="all_papers/KusupatiSBKJV18.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/fastgrnn.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/fastgrnn.pdf>[Slides]</a>");
}
var poster="posters/KusupatiSBKJV18.pdf";
var code="code/fastgrnn.html";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/KusupatiSBKJV18.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=code/fastgrnn.html>[Code]</a>");
}
</script>



<div id="toggleTextKusupatiSBKJV18" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction. Previous approaches have improved accuracy at the expense of prediction costs making them infeasible for resource-constrained and real-time applications. Unitary RNNs have increased accuracy somewhat by restricting the range of the state transition matrix's singular values but have also increased the model size as they require a larger number of hidden units to make up for the loss in expressive power. Gated RNNs have obtained state-of-the-art accuracies by adding extra parameters thereby resulting in even larger models. FastRNN addresses these limitations by adding a residual connection that does not constrain the range of the singular values explicitly and has only two extra scalar parameters. FastGRNN then extends the residual connection to a gate by reusing the RNN matrices to match state-of-the-art gated RNN accuracies but with a 2-4x smaller model. Enforcing FastGRNN's matrices to be low-rank, sparse and quantized resulted in accurate models that could be up to 35x smaller than leading gated and unitary RNNs. This allowed FastGRNN to accurately recognize the "Hey Cortana" wakeword with a 1 KB model and to be deployed on severely resource-constrained IoT microcontrollers too tiny to store other RNN models. FastGRNN's code is available online.</div>
<div id="bib_KusupatiSBKJV18" class="bibtex noshow">
<pre>
@inproceedings{KusupatiSBKJV18,
  author = {Aditya Kusupati and Manish Singh and Kush Bhatia and Ashish Kumar and Prateek Jain and Manik Varma},
  title = {FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network},
  booktitle = {Proceedings of the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2018},
  pages = {9031--9042},
  note = {slides/fastgrnn.pdf},
  url = {all_papers/KusupatiSBKJV18.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: SomaniGJN18 -->
<li >
<b>Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds</b><br>
Raghav Somani, Chirag Gupta, Prateek Jain and Praneeth Netrapalli,<br>
in <i>Proceedings of the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)</i>,
2018.
<br />
<a href="javascript:toggleBibtex('SomaniGJN18')">[BibTeX]</a>
<a id="displayTextSomaniGJN18" href="javascript:toggle('SomaniGJN18');">[Abstract]</a>
<a href="all_papers/SomaniGJN18.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/SomaniGJN18.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/SomaniGJN18.pdf>[Slides]</a>");
}
var poster="posters/SomaniGJN18.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/SomaniGJN18.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextSomaniGJN18" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">This paper studies the problem of sparse regression where the goal is to learn a sparse vector that best optimizes a given objective function. Under the assumption that the objective function satisfies restricted strong convexity (RSC), we analyze orthogonal matching pursuit (OMP), a greedy algorithm that is used heavily in applications, and obtain support recovery result as well as a tight generalization error bound for OMP. Furthermore, we obtain lower bounds for OMP, showing that both our results on support recovery and generalization error are tight up to logarithmic factors. To the best of our knowledge, these support recovery and generalization bounds are the first such matching upper and lower bounds (up to logarithmic factors) for {\em any} sparse regression algorithm under the RSC assumption.</div>
<div id="bib_SomaniGJN18" class="bibtex noshow">
<pre>
@inproceedings{SomaniGJN18,
  author = {Raghav Somani and Chirag Gupta and Prateek Jain and Praneeth Netrapalli},
  title = {Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds},
  booktitle = {Proceedings of the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2018},
  pages = {10837--10847},
  note = {slides/SomaniGJN18.pdf},
  url = {all_papers/SomaniGJN18.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: BhojanapalliBJN18 -->
<li >
<b>Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form</b><br>
Srinadh Bhojanapalli, Nicolas Boumal, Prateek Jain and Praneeth Netrapalli,<br>
in <i>Proceedings of the Annual Conference On Learning Theory (COLT)</i>,
2018.
<br />
<a href="javascript:toggleBibtex('BhojanapalliBJN18')">[BibTeX]</a>
<a id="displayTextBhojanapalliBJN18" href="javascript:toggle('BhojanapalliBJN18');">[Abstract]</a>
<a href="all_papers/BhojanapalliBJN18.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/BhojanapalliBJN18.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/BhojanapalliBJN18.pdf>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextBhojanapalliBJN18" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Semidefinite programs (SDP) are important in learning and combinatorial optimization with numerous applications. In pursuit of low-rank solutions and low complexity algorithms, we consider the Burer--Monteiro factorization approach for solving SDPs. We show that all approximate local optima are global optima for the penalty formulation of appropriately rank-constrained SDPs as long as the number of constraints scales sub-quadratically with the desired rank of the optimal solution. Our result is based on a simple penalty function formulation of the rank-constrained SDP along with a smoothed analysis to avoid worst-case cost matrices. We particularize our results to two applications, namely, Max-Cut and matrix completion.</div>
<div id="bib_BhojanapalliBJN18" class="bibtex noshow">
<pre>
@inproceedings{BhojanapalliBJN18,
  author = {Srinadh Bhojanapalli and Nicolas Boumal and Prateek Jain and Praneeth Netrapalli},
  title = {Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form},
  booktitle = {Proceedings of the Annual Conference On Learning Theory (COLT)},
  year = {2018},
  pages = {3243--3270},
  note = {slides/BhojanapalliBJN18.pdf},
  url = {all_papers/BhojanapalliBJN18.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JKKNS18 -->
<li >
<b>Accelerating Stochastic Gradient Descent for Least Squares Regression</b><br>
Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli and Aaron Sidford,<br>
in <i>Proceedings of the Annual Conference On Learning Theory (COLT)</i>,
2018.
<br />
<a href="javascript:toggleBibtex('JKKNS18')">[BibTeX]</a>
<a id="displayTextJKKNS18" href="javascript:toggle('JKKNS18');">[Abstract]</a>
<a href="all_papers/JKKNS18.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/JKNNS18.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/JKNNS18.pdf>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJKKNS18" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">There is widespread sentiment that it is not possible to effectively utilize fast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy ball) for the purposes of stochastic optimization due to their instability and error accumulation, a notion made precise in d'Aspremont 2008 and Devolder, Glineur, and Nesterov 2014. This work considers these issues for the special case of stochastic approximation for the least squares regression problem, and our main result refutes the conventional wisdom by showing that acceleration can be made robust to statistical errors. In particular, this work introduces an accelerated stochastic gradient method that provably achieves the minimax optimal statistical risk faster than stochastic gradient descent. Critical to the analysis is a sharp characterization of accelerated stochastic gradient descent as a stochastic process. We hope this characterization gives insights towards the broader question of designing simple and effective accelerated stochastic methods for more general convex and non-convex optimization problems.</div>
<div id="bib_JKKNS18" class="bibtex noshow">
<pre>
@inproceedings{JKKNS18,
  author = {Prateek Jain and Sham M. Kakade and Rahul Kidambi and Praneeth Netrapalli and Aaron Sidford},
  title = {Accelerating Stochastic Gradient Descent for Least Squares Regression},
  booktitle = {Proceedings of the Annual Conference On Learning Theory (COLT)},
  year = {2018},
  pages = {545--604},
  note = {slides/JKNNS18.pdf},
  url = {all_papers/JKKNS18.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JTT18 -->
<li >
<b>Differentially Private Matrix Completion Revisited</b><br>
Prateek Jain, Om Dipakbhai Thakkar and Abhradeep Thakurta,<br>
in <i>Proceedings of the 35th International Conference on Machine Learning (ICML)</i>,
2018.
<br />
<a href="javascript:toggleBibtex('JTT18')">[BibTeX]</a>
<a id="displayTextJTT18" href="javascript:toggle('JTT18');">[Abstract]</a>
<a href="all_papers/JTT18.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/JTT18.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/JTT18.pdf>[Slides]</a>");
}
var poster="posters/JTT18.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/JTT18.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJTT18" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We study the problem of privacy-preserving collaborative filtering where the objective is to reconstruct the entire users-items preference matrix using a few observed preferences of users for some of the items. Furthermore, the collaborative filtering algorithm should reconstruct the preference matrix while preserving the privacy of each user. We study this problem in the setting of joint differential privacy where each user computes her own preferences for all the items, without violating privacy of other users' preferences. We provide the first provably differentially private algorithm with formal utility guarantees for this problem. Our algorithm is based on the Frank-Wolfe (FW) method, and consistently estimates the underlying preference matrix as long as the number of users $m$ is $\omega(n^{5/4})$, where $n$ is the number of items, and each user provides her preference for at least $\sqrt{n}$ randomly selected items. We also empirically evaluate our FW-based algorithm on a suite of datasets, and show that our method provides nearly same accuracy as the state-of-the-art non-private algorithm, and outperforms the state-of-the-art private algorithm by as much as 30 percent.</div>
<div id="bib_JTT18" class="bibtex noshow">
<pre>
@inproceedings{JTT18,
  author = {Prateek Jain and Om Dipakbhai Thakkar and Abhradeep Thakurta},
  title = {Differentially Private Matrix Completion Revisited},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  year = {2018},
  pages = {2220--2229},
  note = {slides/JTT18.pdf},
  url = {all_papers/JTT18.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: kalyanMPBJG18 -->
<li >
<b>Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples</b><br>
Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra, Prateek Jain and Sumit Gulwani,<br>
in <i>Proceedings of the International Conference on Learning Representations (ICLR)</i>,
2018.
<br />
<a href="javascript:toggleBibtex('kalyanMPBJG18')">[BibTeX]</a>
<a id="displayTextkalyanMPBJG18" href="javascript:toggle('kalyanMPBJG18');">[Abstract]</a>
<a href="all_papers/kalyanMPBJG18.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/kalyanMPBJG18.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/kalyanMPBJG18.pdf>[Slides]</a>");
}
var poster="posters/kalyanMPBJG18.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/kalyanMPBJG18.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextkalyanMPBJG18" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Synthesizing user-intended programs from a small number of input-output exam-
ples is a challenging problem with several important applications like spreadsheet
manipulation, data wrangling and code refactoring. Existing synthesis systems
either completely rely on deductive logic techniques that are extensively hand-
engineered or on purely statistical models that need massive amounts of data, and in
general fail to provide real-time synthesis on challenging benchmarks. In this work,
we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique
that combines the best of both symbolic logic techniques and statistical models.
Thus, it produces programs that satisfy the provided specifications by construction
and generalize well on unseen examples, similar to data-driven systems. Our
technique effectively utilizes the deductive search framework to reduce the learning
problem of the neural component to a simple supervised learning setup. Further,
this allows us to both train on sparingly available real-world data and still leverage
powerful recurrent neural network encoders. We demonstrate the effectiveness
of our method by evaluating on real-world customer scenarios by synthesizing
accurate programs with up to 12Ã— speed-up compared to state-of-the-art systems.</div>
<div id="bib_kalyanMPBJG18" class="bibtex noshow">
<pre>
@inproceedings{kalyanMPBJG18,
  author = {Ashwin Kalyan and Abhishek Mohta and Oleksandr Polozov and Dhruv Batra and Prateek Jain and Sumit Gulwani},
  title = {Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year = {2018},
  note = {slides/kalyanMPBJG18.pdf},
  url = {all_papers/kalyanMPBJG18.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: KidambiNJK18 -->
<li >
<b>On the insufficiency of existing momentum schemes for Stochastic Optimization</b><br>
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain and Sham M. Kakade,<br>
in <i>Proceedings of the International Conference on Learning Representations (ICLR)</i>,
2018.
<br />
<a href="javascript:toggleBibtex('KidambiNJK18')">[BibTeX]</a>
<a id="displayTextKidambiNJK18" href="javascript:toggle('KidambiNJK18');">[Abstract]</a>
<a href="all_papers/KidambiNJK18.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/KidambiNJK18.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/KidambiNJK18.pdf>[Slides]</a>");
}
var poster="posters/KidambiNJK18.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/KidambiNJK18.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextKidambiNJK18" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Rigorously speaking, fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there exist simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching. 
Furthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Acceleration. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD. The code for implementing the ASGD Algorithm can be found at https://github.com/rahulkidambi/AccSGD.</div>
<div id="bib_KidambiNJK18" class="bibtex noshow">
<pre>
@inproceedings{KidambiNJK18,
  author = {Rahul Kidambi and Praneeth Netrapalli and Prateek Jain and Sham M. Kakade},
  title = {On the insufficiency of existing momentum schemes for Stochastic Optimization},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year = {2018},
  note = {slides/KidambiNJK18.pdf},
  url = {all_papers/KidambiNJK18.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: PadhiJPPGM18 -->
<li >
<b>FlashProfile: a framework for synthesizing data profiles</b><br>
Saswat Padhi, Prateek Jain, Daniel Perelman, Oleksandr Polozov, Sumit Gulwani and Todd D. Millstein,<br>
in <i>Proceedings of Object-Oriented Programming, Systems, Languages and Applications (OOPSLA)</i>,
2018.
<br />
<a href="javascript:toggleBibtex('PadhiJPPGM18')">[BibTeX]</a>
<a id="displayTextPadhiJPPGM18" href="javascript:toggle('PadhiJPPGM18');">[Abstract]</a>
<a href="all_papers/PadhiJPPGM18.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/PadhiJPPGM18.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/PadhiJPPGM18.pdf>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextPadhiJPPGM18" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We address the problem of learning a syntactic profile for a collection of strings, i.e. a set of regex-like patterns
that succinctly describe the syntactic variations in the strings. Real-world datasets, typically curated from
multiple sources, often contain data in various syntactic formats. Thus, any data processing task is preceded
by the critical step of data format identification. However, manual inspection of data to identify the different
formats is infeasible in standard big-data scenarios.
Prior techniques are restricted to a small set of pre-defined patterns (e.g. digits, letters, words, etc.), and
provide no control over granularity of profiles. We define syntactic profiling as a problem of clustering strings
based on syntactic similarity, followed by identifying patterns that succinctly describe each cluster. We present
a technique for synthesizing such profiles over a given language of patterns, that also allows for interactive
refinement by requesting a desired number of clusters.
Using a state-of-the-art inductive synthesis framework, PROSE, we have implemented our technique as
FlashProfile. Across 153 tasks over 75 large real datasets, we observe a median profiling time of only 0.7 s.
Furthermore, we show that access to syntactic profiles may allow for more accurate synthesis of programs, i.e.
using fewer examples, in programming-by-example (PBE) workflows such as Flash Fill.</div>
<div id="bib_PadhiJPPGM18" class="bibtex noshow">
<pre>
@inproceedings{PadhiJPPGM18,
  author = {Saswat Padhi and Prateek Jain and Daniel Perelman and Oleksandr Polozov and Sumit Gulwani and Todd D. Millstein},
  title = {FlashProfile: a framework for synthesizing data profiles},
  booktitle = {Proceedings of Object-Oriented Programming, Systems, Languages and Applications (OOPSLA)},
  year = {2018},
  pages = {150:1--150:28},
  note = {slides/PadhiJPPGM18.pdf},
  url = {all_papers/PadhiJPPGM18.pdf},
  doi = {https://doi.org/10.1145/3276520}
}
</pre>
</div>
</li>
<br /><!-- Item: JainKKNPS17 -->
<li >
<b>A Markov Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares)</b><br>
Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, Venkata Krishna Pillutla and Aaron Sidford,<br>
in <i>Proceedings of the Thirty-seventh IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('JainKKNPS17')">[BibTeX]</a>
<a id="displayTextJainKKNPS17" href="javascript:toggle('JainKKNPS17');">[Abstract]</a>
<a href="all_papers/JKKNPS17.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainKKNPS17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">This work provides a simplified proof of the statistical minimax optimality of (iterate averaged) stochastic gradient descent (SGD), for the special case of least squares. This result is obtained by analyzing SGD as a stochastic process and by sharply characterizing the stationary covariance matrix of this process. The finite rate optimality characterization captures the constant factors and addresses model mis-specification.</div>
<div id="bib_JainKKNPS17" class="bibtex noshow">
<pre>
@inproceedings{JainKKNPS17,
  author = {Prateek Jain and Sham M. Kakade and Rahul Kidambi and Praneeth Netrapalli and Venkata Krishna Pillutla and Aaron Sidford},
  title = {A Markov Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares)},
  booktitle = {Proceedings of the Thirty-seventh IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS)},
  year = {2017},
  pages = {2:1--2:10},
  url = {all_papers/JKKNPS17.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: BhatiaJKK17 -->
<li >
<b>Consistent Robust Regression</b><br>
Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban and Purushottam Kar,<br>
in <i>Proceedings of the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('BhatiaJKK17')">[BibTeX]</a>
<a id="displayTextBhatiaJKK17" href="javascript:toggle('BhatiaJKK17');">[Abstract]</a>
<a href="all_papers/BhatiaJKK17.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="posters/BhatiaJKK17.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/BhatiaJKK17.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextBhatiaJKK17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We present the first efficient and provably consistent estimator for the robust regression problem. The area of robust learning and optimization has generated a significant amount of interest in the learning and statistics communities in recent years owing to its applicability in scenarios with corrupted data, as well as in handling model mis-specifications. In particular, special interest has been devoted to the fundamental problem of robust linear regression where estimators that can tolerate corruption in up to a constant fraction of the response variables are widely studied. Surprisingly however, to this date, we are not aware of a polynomial time estimator that offers a consistent estimate in the presence of dense, unbounded corruptions. In this work we present such an estimator, called CRR. This solves an open problem put forward in the work of (Bhatia et al, 2015). Our consistency analysis requires a novel two-stage proof technique involving a careful analysis of the stability of ordered lists which may be of independent interest. We show that CRR not only offers consistent estimates, but is empirically far superior to several other recently proposed algorithms for the robust regression problem, including extended Lasso and the TORRENT algorithm. In comparison, CRR offers comparable or better model recovery but with runtimes that are faster by an order of magnitude.</div>
<div id="bib_BhatiaJKK17" class="bibtex noshow">
<pre>
@inproceedings{BhatiaJKK17,
  author = {Kush Bhatia and Prateek Jain and Parameswaran Kamalaruban and Purushottam Kar},
  title = {Consistent Robust Regression},
  booktitle = {Proceedings of the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS)},
  year = {2017},
  pages = {2107--2116},
  url = {all_papers/BhatiaJKK17.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: RaghunathanJK17 -->
<li >
<b>Learning Mixture of Gaussians with Streaming Data</b><br>
Aditi Raghunathan, Prateek Jain and Ravishankar Krishnaswamy,<br>
in <i>Proceedings of the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('RaghunathanJK17')">[BibTeX]</a>
<a id="displayTextRaghunathanJK17" href="javascript:toggle('RaghunathanJK17');">[Abstract]</a>
<a href="all_papers/RaghunathanJK17.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="posters/RaghunathanJK17.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/RaghunathanJK17.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextRaghunathanJK17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">In this paper, we study the problem of learning a mixture of Gaussians with streaming data: given a stream of N points in d dimensions generated by an unknown mixture of k spherical Gaussians, the goal is to estimate the model parameters using a single pass over the data stream. We analyze a streaming version of the popular Lloyd's heuristic and show that the algorithm estimates all the unknown centers of the component Gaussians accurately if they are sufficiently separated. Assuming each pair of centers are Csigma distant with C\geq ((klogk)1/4sigma) and where sigma^2 is the maximum variance of any Gaussian component, we show that asymptotically the algorithm estimates the centers optimally (up to constants); our center separation requirement matches the best known result for spherical Gaussians \citep{vempalawang}. For finite samples, we show that a bias term based on the initial estimate decreases at O(1/poly(N)) rate while variance decreases at nearly optimal rate of sigma*2d/N. 
Our analysis requires seeding the algorithm with a good initial estimate of the true cluster centers for which we provide an online PCA based clustering algorithm. Indeed, the asymptotic per-step time complexity of our algorithm is the optimal dâ‹…k while space complexity of our algorithm is O(dklogk). 
In addition to the bias and variance terms which tend to 0, the hard-thresholding based updates of streaming Lloyd's algorithm is agnostic to the data distribution and hence incurs an approximation error that cannot be avoided. However, by using a streaming version of the classical (soft-thresholding-based) EM method that exploits the Gaussian distribution explicitly, we show that for a mixture of two Gaussians the true means can be estimated consistently, with estimation error decreasing at nearly optimal rate, and tending to 0 for N goes to infinity.</div>
<div id="bib_RaghunathanJK17" class="bibtex noshow">
<pre>
@inproceedings{RaghunathanJK17,
  author = {Aditi Raghunathan and Prateek Jain and Ravishankar Krishnaswamy},
  title = {Learning Mixture of Gaussians with Streaming Data},
  booktitle = {Proceedings of the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS)},
  year = {2017},
  pages = {6608--6617},
  url = {all_papers/RaghunathanJK17.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: GulwaniJ17 -->
<li >
<b>Programming by Examples: PL meets ML</b><br>
Sumit Gulwani and Prateek Jain,<br>
in <i>Proceedings of the 15th Asian Symposium on Programming Languages and Systems (APLAS)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('GulwaniJ17')">[BibTeX]</a>
<a id="displayTextGulwaniJ17" href="javascript:toggle('GulwaniJ17');">[Abstract]</a>
<a href="all_papers/GulwaniJ17_APLAS.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextGulwaniJ17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Programming by Examples (PBE) involves synthesizing intended 
programs in an underlying domain-specific language from
example-based specifications. PBE systems are already revolutionizing
the application domain of data wrangling and are set to significantly
impact several other domains including code refactoring. There are three key components in a PBE system.
(i) A search algorithm that can efficiently search for programs that are consistent with the examples
provided by the user.
We leverage a divide-and-conquer-based deductive search paradigm that
inductively reduces the problem of synthesizing a program expression of a certain kind that
satisfies a given specification into sub-problems that refer to
sub-expressions or sub-specifications.
(ii) Program ranking techniques to pick an intended program from among the many that satisfy the examples
provided by the user. We leverage features of the program structure as well of the outputs generated by the program on test inputs.
(iii) User interaction models to facilitate usability and debuggability.
We leverage active-learning techniques based on clustering inputs and synthesizing
multiple programs. Each of these PBE components leverage both symbolic reasoning and heuristics.
We make the case for synthesizing these heuristics from
training data using appropriate machine learning methods.
This can not only lead to better heuristics, but can also enable easier development,
maintenance, and even personalization of a PBE system.</div>
<div id="bib_GulwaniJ17" class="bibtex noshow">
<pre>
@inproceedings{GulwaniJ17,
  author = {Sumit Gulwani and Prateek Jain},
  title = {Programming by Examples: PL meets ML},
  booktitle = {Proceedings of the 15th Asian Symposium on Programming Languages and Systems (APLAS)},
  year = {2017},
  url = {all_papers/GulwaniJ17_APLAS.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: ChaudhuriJN17 -->
<li >
<b>Active Heteroscedastic Regression</b><br>
Kamalika Chaudhuri, Prateek Jain and Nagarajan Natarajan,<br>
in <i>Proceedings of the 34th International Conference on Machine Learning (ICML)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('ChaudhuriJN17')">[BibTeX]</a>
<a id="displayTextChaudhuriJN17" href="javascript:toggle('ChaudhuriJN17');">[Abstract]</a>
<a href="http://proceedings.mlr.press/v70/chaudhuri17a.html">[URL]</a>

<script type="text/javascript">
var slide="slides/test.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/test.pdf>[Slides]</a>");
}
var poster="posters/test.pdf";
var code="code/test.html";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/test.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=code/test.html>[Code]</a>");
}
</script>



<div id="toggleTextChaudhuriJN17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_ChaudhuriJN17" class="bibtex noshow">
<pre>
@inproceedings{ChaudhuriJN17,
  author = {Kamalika Chaudhuri and Prateek Jain and Nagarajan Natarajan},
  title = {Active Heteroscedastic Regression},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year = {2017},
  pages = {694--702},
  note = {slides/test.pdf},
  url = {http://proceedings.mlr.press/v70/chaudhuri17a.html}
}
</pre>
</div>
</li>
<br /><!-- Item: CherapanamjeriGJ17 -->
<li >
<b>Nearly Optimal Robust Matrix Completion</b><br>
Yeshwanth Cherapanamjeri, Kartik Gupta and Prateek Jain,<br>
in <i>Proceedings of the 34th International Conference on Machine Learning (ICML)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('CherapanamjeriGJ17')">[BibTeX]</a>
<a id="displayTextCherapanamjeriGJ17" href="javascript:toggle('CherapanamjeriGJ17');">[Abstract]</a>
<a href="http://proceedings.mlr.press/v70/cherapanamjeri17a.html">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextCherapanamjeriGJ17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_CherapanamjeriGJ17" class="bibtex noshow">
<pre>
@inproceedings{CherapanamjeriGJ17,
  author = {Yeshwanth Cherapanamjeri and Kartik Gupta and Prateek Jain},
  title = {Nearly Optimal Robust Matrix Completion},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year = {2017},
  pages = {797--805},
  url = {http://proceedings.mlr.press/v70/cherapanamjeri17a.html}
}
</pre>
</div>
</li>
<br /><!-- Item: GuptaSGSPKGUVJ17 -->
<li >
<b>ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices</b><br>
Chirag Gupta, Arun Sai Suggala, Ankit Goyal, Harsha Vardhan Simhadri, Bhargavi Paranjape, Ashish Kumar, Saurabh Goyal, Raghavendra Udupa, Manik Varma and Prateek Jain,<br>
in <i>Proceedings of the 34th International Conference on Machine Learning (ICML)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('GuptaSGSPKGUVJ17')">[BibTeX]</a>
<a id="displayTextGuptaSGSPKGUVJ17" href="javascript:toggle('GuptaSGSPKGUVJ17');">[Abstract]</a>
<a href="all_papers/GuptaSGSPKGUVJ17_ICML.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="posters/icml-protonn-poster";
var code="https://github.com/Microsoft/EdgeML";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/icml-protonn-poster>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=https://github.com/Microsoft/EdgeML>[Code]</a>");
}
</script>



<div id="toggleTextGuptaSGSPKGUVJ17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_GuptaSGSPKGUVJ17" class="bibtex noshow">
<pre>
@inproceedings{GuptaSGSPKGUVJ17,
  author = {Chirag Gupta and Arun Sai Suggala and Ankit Goyal and Harsha Vardhan Simhadri and Bhargavi Paranjape and Ashish Kumar and Saurabh Goyal and Raghavendra Udupa and Manik Varma and Prateek Jain},
  title = {ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year = {2017},
  pages = {1331--1340},
  url = {all_papers/GuptaSGSPKGUVJ17_ICML.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: ZhongSJBD17 -->
<li >
<b>Recovery Guarantees for One-hidden-layer Neural Networks</b><br>
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett and Inderjit S. Dhillon,<br>
in <i>Proceedings of the 34th International Conference on Machine Learning (ICML)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('ZhongSJBD17')">[BibTeX]</a>
<a id="displayTextZhongSJBD17" href="javascript:toggle('ZhongSJBD17');">[Abstract]</a>
<a href="all_papers/ZSJBD17_ICML.pdf">[URL]</a>

<script type="text/javascript">
var slide="slides/ZhongSJBD17.pdf";
if(slide==""){
document.write("");
}else{
document.write("<a href=slides/ZhongSJBD17.pdf>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextZhongSJBD17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_ZhongSJBD17" class="bibtex noshow">
<pre>
@inproceedings{ZhongSJBD17,
  author = {Kai Zhong and Zhao Song and Prateek Jain and Peter L. Bartlett and Inderjit S. Dhillon},
  title = {Recovery Guarantees for One-hidden-layer Neural Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year = {2017},
  pages = {4140--4149},
  note = {slides/ZhongSJBD17.pdf},
  url = {all_papers/ZSJBD17_ICML.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: CherapanamjeriJN17 -->
<li >
<b>Thresholding Based Outlier Robust PCA</b><br>
Yeshwanth Cherapanamjeri, Prateek Jain and Praneeth Netrapalli,<br>
in <i>Proceedings of the 30th Conference on Learning Theory (COLT)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('CherapanamjeriJN17')">[BibTeX]</a>
<a id="displayTextCherapanamjeriJN17" href="javascript:toggle('CherapanamjeriJN17');">[Abstract]</a>
<a href="http://proceedings.mlr.press/v65/cherapanamjeri17a.html">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextCherapanamjeriJN17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_CherapanamjeriJN17" class="bibtex noshow">
<pre>
@inproceedings{CherapanamjeriJN17,
  author = {Yeshwanth Cherapanamjeri and Prateek Jain and Praneeth Netrapalli},
  title = {Thresholding Based Outlier Robust PCA},
  booktitle = {Proceedings of the 30th Conference on Learning Theory (COLT)},
  year = {2017},
  pages = {593--628},
  url = {http://proceedings.mlr.press/v65/cherapanamjeri17a.html}
}
</pre>
</div>
</li>
<br /><!-- Item: ZhongJK17 -->
<li >
<b>Fast second-order cone programming for safe mission planning</b><br>
Kai Zhong, Prateek Jain and Ashish Kapoor,<br>
in <i>2017 IEEE International Conference on Robotics and Automation (ICRA)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('ZhongJK17')">[BibTeX]</a>
<a id="displayTextZhongJK17" href="javascript:toggle('ZhongJK17');">[Abstract]</a>
<a href="https://doi.org/10.1109/ICRA.2017.7989014">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextZhongJK17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_ZhongJK17" class="bibtex noshow">
<pre>
@inproceedings{ZhongJK17,
  author = {Kai Zhong and Prateek Jain and Ashish Kapoor},
  title = {Fast second-order cone programming for safe mission planning},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2017},
  pages = {79--86},
  url = {https://doi.org/10.1109/ICRA.2017.7989014},
  doi = {https://doi.org/10.1109/ICRA.2017.7989014}
}
</pre>
</div>
</li>
<br /><!-- Item: JJKN17 -->
<li >
<b>Global Convergence of Non-Convex Gradient Descent for Computing Matrix Squareroot</b><br>
Prateek Jain, Chi Jin, Sham M. Kakade and Praneeth Netrapalli,<br>
in <i>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('JJKN17')">[BibTeX]</a>
<a id="displayTextJJKN17" href="javascript:toggle('JJKN17');">[Abstract]</a>
<a href="http://proceedings.mlr.press/v54/jain17a.html">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJJKN17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JJKN17" class="bibtex noshow">
<pre>
@inproceedings{JJKN17,
  author = {Prateek Jain and Chi Jin and Sham M. Kakade and Praneeth Netrapalli},
  title = {Global Convergence of Non-Convex Gradient Descent for Computing Matrix Squareroot},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2017},
  pages = {479--488},
  url = {http://proceedings.mlr.press/v54/jain17a.html}
}
</pre>
</div>
</li>
<br /><!-- Item: AggarwalGSSRKJ17 -->
<li >
<b>Scalable Optimization of Multivariate Performance Measures in Multi-instance Multi-label Learning</b><br>
Apoorv Aggarwal, Sandip Ghoshal, Ankith M. S. Shetty, Suhit Sinha, Ganesh Ramakrishnan, Purushottam Kar and Prateek Jain,<br>
in <i>Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI)</i>,
2017.
<br />
<a href="javascript:toggleBibtex('AggarwalGSSRKJ17')">[BibTeX]</a>
<a id="displayTextAggarwalGSSRKJ17" href="javascript:toggle('AggarwalGSSRKJ17');">[Abstract]</a>
<a href="all_papers/AggarwalGSSRKJ17.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="posters/AAAI-17_MIML-perf_Poster.pdf";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=posters/AAAI-17_MIML-perf_Poster.pdf>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextAggarwalGSSRKJ17" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_AggarwalGSSRKJ17" class="bibtex noshow">
<pre>
@inproceedings{AggarwalGSSRKJ17,
  author = {Apoorv Aggarwal and Sandip Ghoshal and Ankith M. S. Shetty and Suhit Sinha and Ganesh Ramakrishnan and Purushottam Kar and Prateek Jain},
  title = {Scalable Optimization of Multivariate Performance Measures in Multi-instance Multi-label Learning},
  booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2017},
  pages = {1698--1704},
  url = {all_papers/AggarwalGSSRKJ17.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JRD16 -->
<li >
<b>Structured Sparse Regression via Greedy Hard Thresholding</b><br>
Prateek Jain, Nikhil Rao and Inderjit S. Dhillon,<br>
in <i>Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2016.
<br />
<a href="javascript:toggleBibtex('JRD16')">[BibTeX]</a>
<a id="displayTextJRD16" href="javascript:toggle('JRD16');">[Abstract]</a>
<a href="http://papers.nips.cc/paper/6425-structured-sparse-regression-via-greedy-hard-thresholding">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJRD16" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JRD16" class="bibtex noshow">
<pre>
@inproceedings{JRD16,
  author = {Prateek Jain and Nikhil Rao and Inderjit S. Dhillon},
  title = {Structured Sparse Regression via Greedy Hard Thresholding},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2016},
  pages = {1516--1524},
  url = {http://papers.nips.cc/paper/6425-structured-sparse-regression-via-greedy-hard-thresholding}
}
</pre>
</div>
</li>
<br /><!-- Item: NatarajanJ16 -->
<li >
<b>Regret Bounds for Non-decomposable Metrics with Missing Labels</b><br>
Nagarajan Natarajan and Prateek Jain,<br>
in <i>Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2016.
<br />
<a href="javascript:toggleBibtex('NatarajanJ16')">[BibTeX]</a>
<a id="displayTextNatarajanJ16" href="javascript:toggle('NatarajanJ16');">[Abstract]</a>
<a href="http://papers.nips.cc/paper/6178-regret-bounds-for-non-decomposable-metrics-with-missing-labels">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextNatarajanJ16" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_NatarajanJ16" class="bibtex noshow">
<pre>
@inproceedings{NatarajanJ16,
  author = {Nagarajan Natarajan and Prateek Jain},
  title = {Regret Bounds for Non-decomposable Metrics with Missing Labels},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2016},
  pages = {2874--2882},
  url = {http://papers.nips.cc/paper/6178-regret-bounds-for-non-decomposable-metrics-with-missing-labels}
}
</pre>
</div>
</li>
<br /><!-- Item: YangBJL16 -->
<li >
<b>Selective inference for group-sparse linear models</b><br>
Fan Yang, Rina Foygel Barber, Prateek Jain and John D. Lafferty,<br>
in <i>Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2016.
<br />
<a href="javascript:toggleBibtex('YangBJL16')">[BibTeX]</a>
<a id="displayTextYangBJL16" href="javascript:toggle('YangBJL16');">[Abstract]</a>
<a href="http://papers.nips.cc/paper/6437-selective-inference-for-group-sparse-linear-models">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextYangBJL16" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_YangBJL16" class="bibtex noshow">
<pre>
@inproceedings{YangBJL16,
  author = {Fan Yang and Rina Foygel Barber and Prateek Jain and John D. Lafferty},
  title = {Selective inference for group-sparse linear models},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2016},
  pages = {2469--2477},
  url = {http://papers.nips.cc/paper/6437-selective-inference-for-group-sparse-linear-models}
}
</pre>
</div>
</li>
<br /><!-- Item: ZhongJD16 -->
<li >
<b>Mixed Linear Regression with Multiple Components</b><br>
Kai Zhong, Prateek Jain and Inderjit S. Dhillon,<br>
in <i>Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2016.
<br />
<a href="javascript:toggleBibtex('ZhongJD16')">[BibTeX]</a>
<a id="displayTextZhongJD16" href="javascript:toggle('ZhongJD16');">[Abstract]</a>
<a href="http://papers.nips.cc/paper/6240-mixed-linear-regression-with-multiple-components">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextZhongJD16" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_ZhongJD16" class="bibtex noshow">
<pre>
@inproceedings{ZhongJD16,
  author = {Kai Zhong and Prateek Jain and Inderjit S. Dhillon},
  title = {Mixed Linear Regression with Multiple Components},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2016},
  pages = {2190--2198},
  url = {http://papers.nips.cc/paper/6240-mixed-linear-regression-with-multiple-components}
}
</pre>
</div>
</li>
<br /><!-- Item: RaoJJ16 -->
<li >
<b>Diverse Yet Efficient Retrieval using Locality Sensitive Hashing</b><br>
Vidyadhar Rao, Prateek Jain and C. V. Jawahar,<br>
in <i>Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval (ICMR)</i>,
2016.
<br />
<a href="javascript:toggleBibtex('RaoJJ16')">[BibTeX]</a>
<a id="displayTextRaoJJ16" href="javascript:toggle('RaoJJ16');">[Abstract]</a>
<a href="http://doi.acm.org/10.1145/2911996.2911998">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextRaoJJ16" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_RaoJJ16" class="bibtex noshow">
<pre>
@inproceedings{RaoJJ16,
  author = {Vidyadhar Rao and Prateek Jain and C. V. Jawahar},
  title = {Diverse Yet Efficient Retrieval using Locality Sensitive Hashing},
  booktitle = {Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval (ICMR)},
  year = {2016},
  pages = {189--196},
  url = {http://doi.acm.org/10.1145/2911996.2911998},
  doi = {https://doi.org/10.1145/2911996.2911998}
}
</pre>
</div>
</li>
<br /><!-- Item: JainJKNS16 -->
<li >
<b>Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm</b><br>
Prateek Jain, Chi Jin, Sham M. Kakade, Praneeth Netrapalli and Aaron Sidford,<br>
in <i>Proceedings of the 29th Conference on Learning Theory (COLT)</i>,
2016.
<br />
<a href="javascript:toggleBibtex('JainJKNS16')">[BibTeX]</a>
<a id="displayTextJainJKNS16" href="javascript:toggle('JainJKNS16');">[Abstract]</a>
<a href="http://jmlr.org/proceedings/papers/v49/jain16.html">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainJKNS16" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainJKNS16" class="bibtex noshow">
<pre>
@inproceedings{JainJKNS16,
  author = {Prateek Jain and Chi Jin and Sham M. Kakade and Praneeth Netrapalli and Aaron Sidford},
  title = {Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm},
  booktitle = {Proceedings of the 29th Conference on Learning Theory (COLT)},
  year = {2016},
  pages = {1147--1164},
  url = {http://jmlr.org/proceedings/papers/v49/jain16.html}
}
</pre>
</div>
</li>
<br /><!-- Item: AnandkumarJSN16 -->
<li >
<b>Tensor vs. Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations</b><br>
Anima Anandkumar, Prateek Jain, Yang Shi and U. N. Niranjan,<br>
in <i>Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS)</i>,
2016.
<br />
<a href="javascript:toggleBibtex('AnandkumarJSN16')">[BibTeX]</a>
<a id="displayTextAnandkumarJSN16" href="javascript:toggle('AnandkumarJSN16');">[Abstract]</a>
<a href="http://jmlr.org/proceedings/papers/v51/anandkumar16.html">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextAnandkumarJSN16" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_AnandkumarJSN16" class="bibtex noshow">
<pre>
@inproceedings{AnandkumarJSN16,
  author = {Anima Anandkumar and Prateek Jain and Yang Shi and U. N. Niranjan},
  title = {Tensor vs. Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2016},
  pages = {268--276},
  url = {http://jmlr.org/proceedings/papers/v51/anandkumar16.html}
}
</pre>
</div>
</li>
<br /><!-- Item: ZhongJD15 -->
<li >
<b>Efficient Matrix Sensing Using Rank-1 Gaussian Measurements</b><br>
Kai Zhong, Prateek Jain and Inderjit S. Dhillon,<br>
in <i>Algorithmic Learning Theory - 26th International Conference (ALT)</i>,
2015.
<br />
<a href="javascript:toggleBibtex('ZhongJD15')">[BibTeX]</a>
<a id="displayTextZhongJD15" href="javascript:toggle('ZhongJD15');">[Abstract]</a>
<a href="https://doi.org/10.1007/978-3-319-24486-0_1">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextZhongJD15" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_ZhongJD15" class="bibtex noshow">
<pre>
@inproceedings{ZhongJD15,
  author = {Kai Zhong and Prateek Jain and Inderjit S. Dhillon},
  title = {Efficient Matrix Sensing Using Rank-1 Gaussian Measurements},
  booktitle = {Algorithmic Learning Theory - 26th International Conference (ALT)},
  year = {2015},
  pages = {3--18},
  url = {https://doi.org/10.1007/978-3-319-24486-0_1},
  doi = {https://doi.org/10.1007/978-3-319-24486-0_1}
}
</pre>
</div>
</li>
<br /><!-- Item: BhatiaJK15 -->
<li >
<b>Robust Regression via Hard Thresholding</b><br>
Kush Bhatia, Prateek Jain and Purushottam Kar,<br>
in <i>Proceedings of the 28th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</i>,
2015.
<br />
<a href="javascript:toggleBibtex('BhatiaJK15')">[BibTeX]</a>
<a id="displayTextBhatiaJK15" href="javascript:toggle('BhatiaJK15');">[Abstract]</a>
<a href="all_papers/BJK15_NIPS.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextBhatiaJK15" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix $X\in \R^{p\times n}$ and an underlying model $\bto$, the response vector is generated as $\y=X^T\bto+\b$ where $\b\in \R^n$ is the corruption vector supported over at most $C\cdot n$ coordinates. Existing exact recovery results for RLSR focus solely on $L_1$-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions $\b$ to be selected independently of $X$.

In this work, we study a simple hard-thresholding algorithm called \alg which, under mild conditions on $X$, can recover $\bto$ exactly even if $\b$ corrupts the response variables in an \emph{adversarial} manner, i.e. both the support and entries of $\b$ are selected adversarially after observing $X$ and $\bto$. Our results hold under \emph{deterministic} assumptions which are satisfied if $X$ is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed $\bto$, generated independently of $X$, our results are \emph{universal} and hold for any $\bto\in \R^p$.
Next, we propose gradient descent-based extensions of \alg that can scale efficiently to large scale problems, such as high dimensional sparse recovery, and prove similar recovery guarantees for these extensions. Empirically we find \alg, and more so its extensions, offering significantly faster recovery than the state-of-the-art $L_1$ solvers. Empirically we find \alg, and more so its extensions, offering significantly faster recovery than the state-of-the-art $L_1$ solvers. For instance, even on moderate-sized datasets (with $p=50K$) with around $40\%$ corrupted responses, a variant of our proposed method called \alg-HYB is more than $20\times$ faster than the best $L_1$ solver. 
</div>
<div id="bib_BhatiaJK15" class="bibtex noshow">
<pre>
@inproceedings{BhatiaJK15,
  author = {Kush Bhatia and Prateek Jain and Purushottam Kar},
  title = {Robust Regression via Hard Thresholding},
  booktitle = {Proceedings of the 28th Annual Conference on Advances in Neural Information Processing Systems (NIPS)},
  year = {2015},
  url = {all_papers/BJK15_NIPS.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: BhatiaJKVJ15 -->
<li >
<b>Sparse Local Embeddings for Extreme Multi-label Classification</b><br>
Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma and Prateek Jain,<br>
in <i>Proceedings of the 28th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</i>,
2015.
<br />
<a href="javascript:toggleBibtex('BhatiaJKVJ15')">[BibTeX]</a>
<a id="displayTextBhatiaJKVJ15" href="javascript:toggle('BhatiaJKVJ15');">[Abstract]</a>
<a href="all_papers/BJKVJ15_NIPS.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextBhatiaJKVJ15" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">The objective in extreme multi-label learning is to train a classifier that can automatically
tag a novel data point with the most relevant subset of labels from an
extremely large label set. Embedding based approaches attempt to make training
and prediction tractable by assuming that the training label matrix is low-rank and
reducing the effective number of labels by projecting the high dimensional label
vectors onto a low dimensional linear subspace. Still, leading embedding approaches
have been unable to deliver high prediction accuracies, or scale to large
problems as the low rank assumption is violated in most real world applications.
In this paper we develop the SLEEC classifier to address both limitations. The
main technical contribution in SLEEC is a formulation for learning a small ensemble
of local distance preserving embeddings which can accurately predict infrequently
occurring (tail) labels. This allows SLEEC to break free of the traditional
low-rank assumption and boost classification accuracy by learning embeddings
which preserve pairwise distances between only the nearest label vectors.
We conducted extensive experiments on several real-world, as well as benchmark
data sets and compared our method against state-of-the-art methods for extreme
multi-label classification. Experiments reveal that SLEEC can make significantly
more accurate predictions then the state-of-the-art methods including both
embedding-based (by as much as 35\%) as well as tree-based (by as much as 6\%)
methods. SLEEC can also scale efficiently to data sets with a million labels which
are beyond the pale of leading embedding methods.</div>
<div id="bib_BhatiaJKVJ15" class="bibtex noshow">
<pre>
@inproceedings{BhatiaJKVJ15,
  author = {Kush Bhatia and Himanshu Jain and Purushottam Kar and Manik Varma and Prateek Jain},
  title = {Sparse Local Embeddings for Extreme Multi-label Classification},
  booktitle = {Proceedings of the 28th Annual Conference on Advances in Neural Information Processing Systems (NIPS)},
  year = {2015},
  url = {all_papers/BJKVJ15_NIPS.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JNT15 -->
<li >
<b>Predtron: A Family of Online Algorithms for General Prediction Problems</b><br>
Prateek Jain, Nagarajan Natarajan and Ambuj Tewari,<br>
in <i>Proceedings of the 28th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</i>,
2015.
<br />
<a href="javascript:toggleBibtex('JNT15')">[BibTeX]</a>
<a id="displayTextJNT15" href="javascript:toggle('JNT15');">[Abstract]</a>
<a href="all_papers/JNT15_NIPS.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJNT15" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning. These problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions. We offer a general framework to derive mistake driven online algorithms and associated loss bounds. The key ingredients in our framework are a general loss function, a general vector space representation of predictions, and a notion of margin with respect to a general norm. Our general algorithm, \algoname, yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification, multiclass classification, ordinal regression, and multilabel classification. For multilabel ranking and subset ranking, we derive novel algorithms, notions of margins, and loss bounds. A simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework.
</div>
<div id="bib_JNT15" class="bibtex noshow">
<pre>
@inproceedings{JNT15,
  author = {Prateek Jain and Nagarajan Natarajan and Ambuj Tewari},
  title = {Predtron: A Family of Online Algorithms for General Prediction Problems},
  booktitle = {Proceedings of the 28th Annual Conference on Advances in Neural Information Processing Systems (NIPS)},
  year = {2015},
  url = {all_papers/JNT15_NIPS.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JT15 -->
<li >
<b>Alternating Minimization for Regression Problems with Vector-valued Outputs</b><br>
Prateek Jain and Ambuj Tewari,<br>
in <i>Proceedings of the 28th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</i>,
2015.
<br />
<a href="javascript:toggleBibtex('JT15')">[BibTeX]</a>
<a id="displayTextJT15" href="javascript:toggle('JT15');">[Abstract]</a>
<a href="all_papers/JT15_NIPS.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJT15" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">In regression problems involving vector-valued outputs (or equivalently, multiple responses), it is well known that the maximum likelihood estimator (MLE), which takes noise covariance structure into account, can be significantly more accurate than the ordinary least squares (OLS) estimator. However, existing literature compares OLS and MLE in terms of their asymptotic, not finite sample, guarantees. More crucially, computing the MLE in general requires solving a non-convex optimization problem and is not known to be efficiently solvable. We provide finite sample upper and lower bounds on the estimation error of OLS and MLE, in two popular models: a) Pooled model, b) Seemingly Unrelated Regression (SUR) model. We provide precise instances where the MLE is significantly more accurate than OLS. Furthermore, for both models, we show that the output of a computationally efficient alternating minimization procedure enjoys the same performance guarantee as MLE, up to universal constants. Finally, we show that for high-dimensional settings as well, the alternating minimization procedure leads to significantly more accurate solutions than the corresponding OLS solutions but with error bound that depends only logarithmically on the data dimensionality. 
</div>
<div id="bib_JT15" class="bibtex noshow">
<pre>
@inproceedings{JT15,
  author = {Prateek Jain and Ambuj Tewari},
  title = {Alternating Minimization for Regression Problems with Vector-valued Outputs},
  booktitle = {Proceedings of the 28th Annual Conference on Advances in Neural Information Processing Systems (NIPS)},
  year = {2015},
  url = {all_papers/JT15_NIPS.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: KarNJ15 -->
<li >
<b>Surrogate Functions for Maximizing Precision at the Top</b><br>
Purushottam Kar, Harikrishna Narasimhan and Prateek Jain,<br>
in <i>Proceedings of the 32nd International Conference on Machine Learning (ICML)</i>,
2015.
<br />
<a href="javascript:toggleBibtex('KarNJ15')">[BibTeX]</a>
<a id="displayTextKarNJ15" href="javascript:toggle('KarNJ15');">[Abstract]</a>
<a href="all_papers/KNJ15_ICML.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextKarNJ15" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">The problem of maximizing precision at top, also dubbed Precision@k, finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalances. Despite its popularity, Precision@k is not known to have a surrogate function that upper bounds it. Similarly, notions of consistency under certain noise/margin conditions are also not explored.

In this work, we devise two novel convex surrogate functions for Precision@k, that upper bound it and are motivated by certain natural notions of margin for Precision@k performance measure. We also provide two novel perceptron algorithms for Precision@k that have interesting mistake bounds w.r.t. the proposed surrogates.

Moreover, we devise scalable stochastic gradient descent style methods for our proposed surrogates and prove convergence bounds for the same. Our convergence bounds rely on a strong uniform convergence bound for Precsion@k and crucially exploit the structural simplicity of Precision@k. We conclude with experimental evidence of superiority of our surrogates when compared to the structural SVM surrogate \cite{Joachims05}, a state-of-the-art approach to optimize Precision@k.</div>
<div id="bib_KarNJ15" class="bibtex noshow">
<pre>
@inproceedings{KarNJ15,
  author = {Purushottam Kar and Harikrishna Narasimhan and Prateek Jain},
  title = {Surrogate Functions for Maximizing Precision at the Top},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
  year = {2015},
  pages = {189--198},
  url = {all_papers/KNJ15_ICML.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: NarasimhanKJ15 -->
<li >
<b>Optimizing Non-decomposable Performance Measures: A Tale of Two Classes</b><br>
Harikrishna Narasimhan, Purushottam Kar and Prateek Jain,<br>
in <i>Proceedings of the 32nd International Conference on Machine Learning (ICML)</i>,
2015.
<br />
<a href="javascript:toggleBibtex('NarasimhanKJ15')">[BibTeX]</a>
<a id="displayTextNarasimhanKJ15" href="javascript:toggle('NarasimhanKJ15');">[Abstract]</a>
<a href="all_papers/NKJ15_ICML.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextNarasimhanKJ15" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics, and require optimizing performance measures that are non-decomposable over the dataset, such as F-measure. Such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent.

In this paper we reveal that for two large families of performance measures that can be expressed as functions of true positive/negative rates, it is indeed possible to implement point stochastic updates. The families we consider are concave and pseudo-linear functions of TPR, TNR which cover several popularly used performance measures such as F-measure, G-mean and H-mean.

Our core contribution is an adaptive linearization scheme for these families, using which we develop optimization techniques that enable truly point-based stochastic updates. For concave performance measures we propose \pdsgd, a stochastic primal dual solver; for pseudo-linear measures we propose \amsgd, a stochastic alternate maximization procedure. Both methods have crisp convergence guarantees, demonstrate significant speedups over existing methods - often by an order of magnitude or more, and give similar or more accurate predictions on test data.
</div>
<div id="bib_NarasimhanKJ15" class="bibtex noshow">
<pre>
@inproceedings{NarasimhanKJ15,
  author = {Harikrishna Narasimhan and Purushottam Kar and Prateek Jain},
  title = {Optimizing Non-decomposable Performance Measures: A Tale of Two Classes},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
  year = {2015},
  pages = {199--208},
  url = {all_papers/NKJ15_ICML.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JN15 -->
<li >
<b>Fast Exact Matrix Completion with Finite Samples</b><br>
Prateek Jain and Praneeth Netrapalli,<br>
in <i>Proceedings of The 28th Conference on Learning Theory (COLT)</i>,
2015.
<br />
<a href="javascript:toggleBibtex('JN15')">[BibTeX]</a>
<a id="displayTextJN15" href="javascript:toggle('JN15');">[Abstract]</a>
<a href="all_papers/JN15_COLT.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJN15" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Matrix completion is the problem of recovering a low rank matrix by observing a small fraction of its entries. A series of recent works \citep{Keshavan2012,JainNS2013,Hardt2013} have proposed fast non-convex optimization based iterative algorithms to solve this problem. However, the sample complexity in all these results is sub-optimal in its dependence on the rank, condition number and the desired accuracy.

In this paper, we present a fast iterative algorithm that solves the matrix completion problem by observing $\order{nr^5 \log^3 n}$ entries, which is independent of the condition number and the desired accuracy. The run time of our algorithm is $\order{nr^7\log^3 n\log 1/\epsilon}$ which is near linear in the dimension of the matrix. To the best of our knowledge, this is the first near linear time algorithm for exact matrix completion with finite sample complexity (i.e. independent of $\epsilon$). 

Our algorithm is based on a well known projected gradient descent method, where the projection is onto the (non-convex) set of low rank matrices. There are two key ideas in our result: 1) our argument is based on a $\ell_\infty$ norm potential function (as opposed to the spectral norm) and provides a novel way to obtain perturbation bounds for it. 2) we prove and use a natural extension of the Davis-Kahan theorem to obtain perturbation bounds on the best low rank approximation of matrices with good eigen gap. Both of these ideas may be of independent interest.</div>
<div id="bib_JN15" class="bibtex noshow">
<pre>
@inproceedings{JN15,
  author = {Prateek Jain and Praneeth Netrapalli},
  title = {Fast Exact Matrix Completion with Finite Samples},
  booktitle = {Proceedings of The 28th Conference on Learning Theory (COLT)},
  year = {2015},
  pages = {1007--1034},
  url = {all_papers/JN15_COLT.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: BhojanapalliJS15 -->
<li >
<b>Tighter Low-rank Approximation via Sampling the Leveraged Element</b><br>
Srinadh Bhojanapalli, Prateek Jain and Sujay Sanghavi,<br>
in <i>Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)</i>,
2015.
<br />
<a href="javascript:toggleBibtex('BhojanapalliJS15')">[BibTeX]</a>
<a id="displayTextBhojanapalliJS15" href="javascript:toggle('BhojanapalliJS15');">[Abstract]</a>
<a href="all_papers/BJS15_SODA.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextBhojanapalliJS15" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">In this work, we propose a new randomized algorithm for computing a low-rank approximation to a given matrix. Taking an approach different from existing literature, our method first involves a specific biased sampling, with an element being chosen based on the leverage scores of its row and column, and then involves weighted alternating minimization over the factored form of the intended low-rank matrix, to minimize error only on these samples. Our method can leverage input sparsity, yet produce approximations in {\em spectral} (as opposed to the weaker Frobenius) norm; this combines the best aspects of otherwise disparate current results, but with a dependence on the condition number $\kappa = \sigma_1/\sigma_r$. In particular we require $O(nnz(M) + \frac{n\kappa^2 r^5}{\epsilon^2} )$ computations to generate a rank-$r$ approximation to $M$ in spectral norm. In contrast, the best existing method requires $O(nnz(M)+ \frac{nr^2}{\epsilon^4})$ time to compute an approximation in Frobenius norm. Besides the tightness in spectral norm, we have a better dependence on the error $\epsilon$. Our method is naturally and highly parallelizable.

Our new approach enables two extensions that are interesting on their own. The first is a new method to directly compute a low-rank approximation (in efficient factored form) to the product of two given matrices; it computes a small random set of entries of the product, and then executes weighted alternating minimization (as before) on these. The sampling strategy is different because now we cannot access leverage scores of the product matrix (but instead have to work with input matrices). The second extension is an improved algorithm with smaller communication complexity for the distributed PCA setting (where each server has small set of rows of the matrix, and want to compute low rank approximation with small amount of communication with other servers).</div>
<div id="bib_BhojanapalliJS15" class="bibtex noshow">
<pre>
@inproceedings{BhojanapalliJS15,
  author = {Srinadh Bhojanapalli and Prateek Jain and Sujay Sanghavi},
  title = {Tighter Low-rank Approximation via Sampling the Leveraged Element},
  booktitle = {Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)},
  year = {2015},
  pages = {902--920},
  url = {all_papers/BJS15_SODA.pdf},
  doi = {https://doi.org/10.1137/1.9781611973730.62}
}
</pre>
</div>
</li>
<br /><!-- Item: KarNJ14 -->
<li >
<b>Online and Stochastic Gradient Methods for Non-decomposable Loss Functions</b><br>
Purushottam Kar, Harikrishna Narasimhan and Prateek Jain,<br>
in <i>Proceedings of the 27th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</i>,
2014.
<br />
<a href="javascript:toggleBibtex('KarNJ14')">[BibTeX]</a>
<a id="displayTextKarNJ14" href="javascript:toggle('KarNJ14');">[Abstract]</a>
<a href="all_papers/NKJ14_NIPS.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextKarNJ14" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Modern applications in sensitive domains such as biometrics and medicine frequently require the use of \emph{non-decomposable} loss functions such as precision$@k$, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, \preck and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes \preck, pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method.</div>
<div id="bib_KarNJ14" class="bibtex noshow">
<pre>
@inproceedings{KarNJ14,
  author = {Purushottam Kar and Harikrishna Narasimhan and Prateek Jain},
  title = {Online and Stochastic Gradient Methods for Non-decomposable Loss Functions},
  booktitle = {Proceedings of the 27th Annual Conference on Advances in Neural Information Processing Systems (NIPS)},
  year = {2014},
  pages = {694--702},
  url = {all_papers/NKJ14_NIPS.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: NetrapalliNSAJ14 -->
<li >
<b>Non-convex Robust PCA</b><br>
Praneeth Netrapalli, U. N. Niranjan, Sujay Sanghavi, Animashree Anandkumar and Prateek Jain,<br>
in <i>Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2014.
<br />
<a href="javascript:toggleBibtex('NetrapalliNSAJ14')">[BibTeX]</a>
<a id="displayTextNetrapalliNSAJ14" href="javascript:toggle('NetrapalliNSAJ14');">[Abstract]</a>
<a href="http://papers.nips.cc/paper/5430-non-convex-robust-pca">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="code/ncrpca.zip";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=code/ncrpca.zip>[Code]</a>");
}
</script>



<div id="toggleTextNetrapalliNSAJ14" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_NetrapalliNSAJ14" class="bibtex noshow">
<pre>
@inproceedings{NetrapalliNSAJ14,
  author = {Praneeth Netrapalli and U. N. Niranjan and Sujay Sanghavi and Animashree Anandkumar and Prateek Jain},
  title = {Non-convex Robust PCA},
  booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2014},
  pages = {1107--1115},
  url = {http://papers.nips.cc/paper/5430-non-convex-robust-pca}
}
</pre>
</div>
</li>
<br /><!-- Item: BhojanapalliJ14 -->
<li >
<b>Universal Matrix Completion</b><br>
Srinadh Bhojanapalli and Prateek Jain,<br>
in <i>Proceedings of the 31th International Conference on Machine Learning (ICML)</i>,
2014.
<br />
<a href="javascript:toggleBibtex('BhojanapalliJ14')">[BibTeX]</a>
<a id="displayTextBhojanapalliJ14" href="javascript:toggle('BhojanapalliJ14');">[Abstract]</a>
<a href="all_papers/BJ14_ICML.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextBhojanapalliJ14" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">The problem of low-rank matrix completion has recently generated a lot of interest leading to several results that offer exact solutions to the problem.  However, in order to do so, these methods make assumptions that can be quite restrictive in practice. More specifically, the methods assume that: a) the observed indices are sampled uniformly at random, and b) for every new matrix, the observed indices are sampled \emph{afresh}. In this work, we address these issues by providing a universal recovery guarantee for matrix completion that works for a variety of sampling schemes. In particular, we show that if the set of sampled indices come from the edges of a bipartite graph with large spectral gap (i.e. gap between the first and the second singular value), then the nuclear norm minimization based method exactly recovers all low-rank matrices that satisfy certain incoherence properties. 
Moreover, we also show that under certain stricter incoherence conditions, $O(nr^2)$ uniformly sampled entries are enough to recover any rank-$r$ $n\times n$ matrix, in contrast to the $O(nr\log n)$ sample complexity required by other matrix completion algorithms as well as existing analyses of the nuclear norm method.</div>
<div id="bib_BhojanapalliJ14" class="bibtex noshow">
<pre>
@inproceedings{BhojanapalliJ14,
  author = {Srinadh Bhojanapalli and Prateek Jain},
  title = {Universal Matrix Completion},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning (ICML)},
  year = {2014},
  pages = {1881--1889},
  url = {all_papers/BJ14_ICML.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JainT14 -->
<li >
<b>(Near) Dimension Independent Risk Bounds for Differentially Private Learning</b><br>
Prateek Jain and Abhradeep Guha Thakurta,<br>
in <i>Proceedings of the 31th International Conference on Machine Learning (ICML)</i>,
2014.
<br />
<a href="javascript:toggleBibtex('JainT14')">[BibTeX]</a>
<a id="displayTextJainT14" href="javascript:toggle('JainT14');">[Abstract]</a>
<a href="all_papers/JT14_ICML.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainT14" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainT14" class="bibtex noshow">
<pre>
@inproceedings{JainT14,
  author = {Prateek Jain and Abhradeep Guha Thakurta},
  title = {(Near) Dimension Independent Risk Bounds for Differentially Private Learning},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning (ICML)},
  year = {2014},
  pages = {476--484},
  url = {all_papers/JT14_ICML.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: YuJKD14 -->
<li >
<b>Large-scale Multi-label Learning with Missing Labels</b><br>
Hsiang-Fu Yu, Prateek Jain, Purushottam Kar and Inderjit S. Dhillon,<br>
in <i>Proceedings of the 31th International Conference on Machine Learning (ICML)</i>,
2014.
<br />
<a href="javascript:toggleBibtex('YuJKD14')">[BibTeX]</a>
<a id="displayTextYuJKD14" href="javascript:toggle('YuJKD14');">[Abstract]</a>
<a href="all_papers/YJKD14_ICML.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextYuJKD14" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">The multi-label classification problem has generated significant interest in recent Years. However, existing approaches do not adequately address two key challenges: (a) scaling up to problems with a large number (say millions) of labels, and (b) handling data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions - such as the squared loss function - to obtain efficient algorithms. We further show that our learning framework admits excess risk bounds even in the presence of missing labels. Our bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as a Wikipedia dataset that has more than 200,000 labels.</div>
<div id="bib_YuJKD14" class="bibtex noshow">
<pre>
@inproceedings{YuJKD14,
  author = {Hsiang-Fu Yu and Prateek Jain and Purushottam Kar and Inderjit S. Dhillon},
  title = {Large-scale Multi-label Learning with Missing Labels},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning (ICML)},
  year = {2014},
  pages = {593--601},
  url = {all_papers/YJKD14_ICML.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: AgarwalA0NT14 -->
<li >
<b>Learning Sparsely Used Overcomplete Dictionaries</b><br>
Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli and Rashish Tandon,<br>
in <i>Proceedings of The 27th Conference on Learning Theory (COLT)</i>,
2014.
<br />
<a href="javascript:toggleBibtex('AgarwalA0NT14')">[BibTeX]</a>
<a id="displayTextAgarwalA0NT14" href="javascript:toggle('AgarwalA0NT14');">[Abstract]</a>
<a href="all_papers/AAJNT14_COLT.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextAgarwalA0NT14" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We consider the problem of learning sparsely used overcomplete dictionaries, where each observation
is a sparse combination of elements from an unknown overcomplete dictionary. We establish
exact recovery when the dictionary elements are mutually incoherent. Our method consists of a
clustering-based initialization step, which provides an approximate estimate of the true dictionary
with guaranteed accuracy. This estimate is then refined via an iterative algorithm with the following
alternating steps: 1) estimation of the dictionary coefficients for each observation through $\ell_1$
minimization, given the dictionary estimate, and 2) estimation of the dictionary elements through
least squares, given the coefficient estimates. We establish that, under a set of sufficient conditions,
our method converges at a linear rate to the true dictionary as well as the true coefficients for each
observation.</div>
<div id="bib_AgarwalA0NT14" class="bibtex noshow">
<pre>
@inproceedings{AgarwalA0NT14,
  author = {Alekh Agarwal and Animashree Anandkumar and Prateek Jain and Praneeth Netrapalli and Rashish Tandon},
  title = {Learning Sparsely Used Overcomplete Dictionaries},
  booktitle = {Proceedings of The 27th Conference on Learning Theory (COLT)},
  year = {2014},
  pages = {123--137},
  url = {all_papers/AAJNT14_COLT.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JainO14 -->
<li >
<b>Learning Mixtures of Discrete Product Distributions using Spectral Decompositions</b><br>
Prateek Jain and Sewoong Oh,<br>
in <i>Proceedings of The 27th Conference on Learning Theory (COLT)</i>,
2014.
<br />
<a href="javascript:toggleBibtex('JainO14')">[BibTeX]</a>
<a id="displayTextJainO14" href="javascript:toggle('JainO14');">[Abstract]</a>
<a href="all_papers/JO14_COLT.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainO14" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We study the problem of learning a distribution from samples, when the underlying distribution is a mixture of product distributions over discrete domains. This problem is motivated by several practical applications such as  crowdsourcing, recommendation systems, and learning Boolean functions. The existing solutions either heavily rely on the fact that the number of mixtures is finite or have  sample/time complexity that is exponential in the number of mixtures.  In this paper, we introduce a polynomial time/sample complexity  method for learning a mixture of $r$ discrete product distributions over $\{1, 2, \dots, \ell\}^n$, for  general $\ell$ and $r$. We show that our approach is consistent and further provide finite sample guarantees. 

We use recently developed techniques from tensor decompositions for moment matching. A crucial step in these approaches is to construct certain  tensors  with low-rank spectral decompositions.These tensors are typically estimated from the sample moments. The main challenge in learning mixtures of discrete product distributions is that the corresponding low-rank tensors cannot be obtained directly from the sample moments. Instead, we need to estimate a low-rank matrix using only off-diagonal entries, and estimate a tensor using a few linear measurements. We give an alternating minimization based method to estimate the low-rank matrix, and  formulate the tensor estimation problem as a least-squares problem.</div>
<div id="bib_JainO14" class="bibtex noshow">
<pre>
@inproceedings{JainO14,
  author = {Prateek Jain and Sewoong Oh},
  title = {Learning Mixtures of Discrete Product Distributions using Spectral Decompositions},
  booktitle = {Proceedings of The 27th Conference on Learning Theory (COLT)},
  year = {2014},
  pages = {824--856},
  url = {all_papers/JO14_COLT.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: MitliagkasC013 -->
<li >
<b>Memory Limited, Streaming PCA</b><br>
Ioannis Mitliagkas, Constantine Caramanis and Prateek Jain,<br>
in <i>Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2013.
<br />
<a href="javascript:toggleBibtex('MitliagkasC013')">[BibTeX]</a>
<a id="displayTextMitliagkasC013" href="javascript:toggle('MitliagkasC013');">[Abstract]</a>
<a href="http://papers.nips.cc/paper/5035-memory-limited-streaming-pca">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextMitliagkasC013" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">We consider streaming, one-pass principal component analysis (PCA), in the high-dimensional regime, with limited memory. Here, $p$-dimensional samples are presented sequentially, and the goal is to produce the $k$-dimensional subspace that best approximates these points. Standard algorithms require $O(p^2)$ memory; meanwhile no algorithm can do better than $O(kp)$ memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the {\em spiked covariance model}, where $p$-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, $n$, scales proportionally with the dimension, $p$. Yet, all algorithms that provably achieve this, have memory complexity $O(p^2)$. Meanwhile, algorithms with memory-complexity $O(kp)$ do not have provable bounds on sample complexity comparable to $p$. We present an algorithm that achieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is able to compute the $k$-dimensional spike with $O(p \log p)$ sample-complexity -- the first algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data.</div>
<div id="bib_MitliagkasC013" class="bibtex noshow">
<pre>
@inproceedings{MitliagkasC013,
  author = {Ioannis Mitliagkas and Constantine Caramanis and Prateek Jain},
  title = {Memory Limited, Streaming PCA},
  booktitle = {Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2013},
  pages = {2886--2894},
  url = {http://papers.nips.cc/paper/5035-memory-limited-streaming-pca}
}
</pre>
</div>
</li>
<br /><!-- Item: NetrapalliJS13 -->
<li >
<b>Phase Retrieval using Alternating Minimization</b><br>
Praneeth Netrapalli, Prateek Jain and Sujay Sanghavi,<br>
in <i>Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2013.
<br />
<a href="javascript:toggleBibtex('NetrapalliJS13')">[BibTeX]</a>
<a id="displayTextNetrapalliJS13" href="javascript:toggle('NetrapalliJS13');">[Abstract]</a>
<a href="all_papers/NJS13_NIPS.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextNetrapalliJS13" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">Phase retrieval problems involve solving linear equations, but with missing  sign (or phase, for complex numbers).
Over the last two decades, a popular generic empirical approach to the many variants of this problem has been one of alternating minimization;
i.e. alternating between estimating the missing phase information, and the candidate solution. In this paper, we show that a simple alternating
minimization algorithm geometrically converges to the solution of one such problem  -- finding a vector $\vecfont{x}$ from $\vecfont{y},\mat{A}$,
where $\vecfont{y} = |\mat{A}^T\vecfont{x}|$ and $|\vecfont{z}|$ denotes a vector of element-wise magnitudes of $\vecfont{z}$ -- under the assumption that $\mat{A}$ is Gaussian.

Empirically, our algorithm performs similar to recently proposed convex techniques for this variant (which are based on ``lifting" to a convex matrix problem) in sample complexity and robustness to noise.
However, our algorithm is much more efficient and can scale to large problems.
Analytically, we show geometric convergence to the solution, and sample complexity that is off by log factors from obvious lower bounds.
We also establish close to optimal scaling for the case when the unknown vector is sparse. Our work represents the only known
theoretical guarantee for alternating minimization for any variant of phase retrieval problems in the non-convex setting.</div>
<div id="bib_NetrapalliJS13" class="bibtex noshow">
<pre>
@inproceedings{NetrapalliJS13,
  author = {Praneeth Netrapalli and Prateek Jain and Sujay Sanghavi},
  title = {Phase Retrieval using Alternating Minimization},
  booktitle = {Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2013},
  pages = {2796--2804},
  url = {all_papers/NJS13_NIPS.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: GopiN0N13 -->
<li >
<b>One-Bit Compressed Sensing: Provable Support and Vector Recovery</b><br>
Sivakant Gopi, Praneeth Netrapalli, Prateek Jain and Aditya V. Nori,<br>
in <i>Proceedings of the 30th International Conference on Machine Learning (ICML)</i>,
2013.
<br />
<a href="javascript:toggleBibtex('GopiN0N13')">[BibTeX]</a>
<a id="displayTextGopiN0N13" href="javascript:toggle('GopiN0N13');">[Abstract]</a>
<a href="http://jmlr.org/proceedings/papers/v28/gopi13.html">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextGopiN0N13" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_GopiN0N13" class="bibtex noshow">
<pre>
@inproceedings{GopiN0N13,
  author = {Sivakant Gopi and Praneeth Netrapalli and Prateek Jain and Aditya V. Nori},
  title = {One-Bit Compressed Sensing: Provable Support and Vector Recovery},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
  year = {2013},
  pages = {154--162},
  url = {http://jmlr.org/proceedings/papers/v28/gopi13.html}
}
</pre>
</div>
</li>
<br /><!-- Item: JainT13 -->
<li >
<b>Differentially Private Learning with Kernels</b><br>
Prateek Jain and Abhradeep Thakurta,<br>
in <i>Proceedings of the 30th International Conference on Machine Learning (ICML)</i>,
2013.
<br />
<a href="javascript:toggleBibtex('JainT13')">[BibTeX]</a>
<a id="displayTextJainT13" href="javascript:toggle('JainT13');">[Abstract]</a>
<a href="all_papers/JT13_ICML.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainT13" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainT13" class="bibtex noshow">
<pre>
@inproceedings{JainT13,
  author = {Prateek Jain and Abhradeep Thakurta},
  title = {Differentially Private Learning with Kernels},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
  year = {2013},
  pages = {118--126},
  url = {all_papers/JT13_ICML.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: KarS0K13 -->
<li >
<b>On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions</b><br>
Purushottam Kar, Bharath K. Sriperumbudur, Prateek Jain and Harish Karnick,<br>
in <i>Proceedings of the 30th International Conference on Machine Learning (ICML)</i>,
2013.
<br />
<a href="javascript:toggleBibtex('KarS0K13')">[BibTeX]</a>
<a id="displayTextKarS0K13" href="javascript:toggle('KarS0K13');">[Abstract]</a>
<a href="all_papers/SKJK13_ICML.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextKarS0K13" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">In this paper, we study the generalization properties of online learning based
stochastic methods for supervised learning problems where the loss function is
dependent on more than one training sample (e.g., metric learning, ranking). We
present a generic decoupling technique that enables us to provide Rademacher
complexity-based generalization error bounds. Our bounds are in general tighter
than those obtained by \citet{online-batch-pairwise} for the same problem. Using
our decoupling technique, we are further able to obtain fast convergence rates
for strongly convex pairwise loss functions. We are also able to analyze a class
of memory efficient online learning algorithms for pairwise learning problems that use
only a bounded subset of past training samples to update the hypothesis at each
step. Finally, in order to complement our generalization bounds, we propose a
novel memory efficient online learning algorithm for higher order learning
problems with bounded regret guarantees.</div>
<div id="bib_KarS0K13" class="bibtex noshow">
<pre>
@inproceedings{KarS0K13,
  author = {Purushottam Kar and Bharath K. Sriperumbudur and Prateek Jain and Harish Karnick},
  title = {On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
  year = {2013},
  pages = {441--449},
  url = {all_papers/SKJK13_ICML.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JainNS13 -->
<li >
<b>Low-rank matrix completion using alternating minimization</b><br>
Prateek Jain, Praneeth Netrapalli and Sujay Sanghavi,<br>
in <i>Proceedings of the Symposium on Theory of Computing Conference (STOC)</i>,
2013.
<br />
<a href="javascript:toggleBibtex('JainNS13')">[BibTeX]</a>
<a id="displayTextJainNS13" href="javascript:toggle('JainNS13');">[Abstract]</a>
<a href="http://doi.acm.org/10.1145/2488608.2488693">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainNS13" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainNS13" class="bibtex noshow">
<pre>
@inproceedings{JainNS13,
  author = {Prateek Jain and Praneeth Netrapalli and Sujay Sanghavi},
  title = {Low-rank matrix completion using alternating minimization},
  booktitle = {Proceedings of the Symposium on Theory of Computing Conference (STOC)},
  year = {2013},
  pages = {665--674},
  url = {http://doi.acm.org/10.1145/2488608.2488693},
  doi = {https://doi.org/10.1145/2488608.2488693}
}
</pre>
</div>
</li>
<br /><!-- Item: NathMJGL13 -->
<li >
<b>Ad impression forecasting for sponsored search</b><br>
Abhirup Nath, Shibnath Mukherjee, Prateek Jain, Navin Goyal and Srivatsan Laxman,<br>
in <i>Proceedings of the 22nd International World Wide Web Conference (WWW)</i>,
2013.
<br />
<a href="javascript:toggleBibtex('NathMJGL13')">[BibTeX]</a>
<a id="displayTextNathMJGL13" href="javascript:toggle('NathMJGL13');">[Abstract]</a>
<a href="all_papers/NMJGL13_WWW.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextNathMJGL13" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">A typical problem for a search engine (hosting sponsored search
service) is to provide the advertisers with a forecast of the number
of impressions his/her ad is likely to obtain for a given bid.
Accurate forecasts have high business value, since they enable advertisers
to select bids that lead to better returns on their investment.
They also play an important role in services such as automatic
campaign optimization. Despite its importance the problem
has remained relatively unexplored in literature. Existing methods
typically overfit to the training data, leading to inconsistent performance.
Furthermore, some of the existing methods cannot provide
predictions for new ads, i.e., for ads that are not present in
the logs. In this paper, we develop a generative model based approach
that addresses these drawbacks. We design a Bayes net to
capture inter-dependencies between the query traffic features and
the competitors in an auction. Furthermore, we account for variability
in the volume of query traffic by using a dynamic linear
model. Finally, we implement our approach on a production grade
MapReduce framework and conduct extensive large scale experiments
on substantial volumes of sponsored search data from Bing.
Our experimental results demonstrate significant advantages over
existing methods as measured using several accuracy/error criteria,
improved ability to provide estimates for new ads and more consistent
performance with smaller variance in accuracies. Our method
can also be adapted to several other related forecasting problems
such as predicting average position of ads or the number of clicks
under budget constraints.</div>
<div id="bib_NathMJGL13" class="bibtex noshow">
<pre>
@inproceedings{NathMJGL13,
  author = {Abhirup Nath and Shibnath Mukherjee and Prateek Jain and Navin Goyal and Srivatsan Laxman},
  title = {Ad impression forecasting for sponsored search},
  booktitle = {Proceedings of the 22nd International World Wide Web Conference (WWW)},
  year = {2013},
  pages = {943--952},
  url = {all_papers/NMJGL13_WWW.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: KapoorVJ12 -->
<li >
<b>Multilabel Classification using Bayesian Compressed Sensing</b><br>
Ashish Kapoor, Raajay Viswanathan and Prateek Jain,<br>
in <i>Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2012.
<br />
<a href="javascript:toggleBibtex('KapoorVJ12')">[BibTeX]</a>
<a id="displayTextKapoorVJ12" href="javascript:toggle('KapoorVJ12');">[Abstract]</a>
<a href="http://books.nips.cc/papers/files/nips25/NIPS2012_1243.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextKapoorVJ12" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_KapoorVJ12" class="bibtex noshow">
<pre>
@inproceedings{KapoorVJ12,
  author = {Ashish Kapoor and Raajay Viswanathan and Prateek Jain},
  title = {Multilabel Classification using Bayesian Compressed Sensing},
  booktitle = {Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2012},
  pages = {2654--2662},
  url = {http://books.nips.cc/papers/files/nips25/NIPS2012_1243.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: KarJ12 -->
<li >
<b>Supervised Learning with Similarity Functions</b><br>
Purushottam Kar and Prateek Jain,<br>
in <i>Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2012.
<br />
<a href="javascript:toggleBibtex('KarJ12')">[BibTeX]</a>
<a id="displayTextKarJ12" href="javascript:toggle('KarJ12');">[Abstract]</a>
<a href="http://books.nips.cc/papers/files/nips25/NIPS2012_0123.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextKarJ12" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_KarJ12" class="bibtex noshow">
<pre>
@inproceedings{KarJ12,
  author = {Purushottam Kar and Prateek Jain},
  title = {Supervised Learning with Similarity Functions},
  booktitle = {Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2012},
  pages = {215--223},
  url = {http://books.nips.cc/papers/files/nips25/NIPS2012_0123.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: HossainPLJBR12 -->
<li >
<b>Improved multiple sequence alignments using coupled pattern mining</b><br>
K. S. M. Tozammel Hossain, Debprakash Patnaik, Srivatsan Laxman, Prateek Jain, Chris Bailey-Kellogg and Naren Ramakrishnan,<br>
in <i>Proceedings of the ACM International Conference on Bioinformatics, Computational Biology and Biomedicine (BCB)</i>,
2012.
<br />
<a href="javascript:toggleBibtex('HossainPLJBR12')">[BibTeX]</a>
<a id="displayTextHossainPLJBR12" href="javascript:toggle('HossainPLJBR12');">[Abstract]</a>
<a href="http://doi.acm.org/10.1145/2382936.2382940">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextHossainPLJBR12" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_HossainPLJBR12" class="bibtex noshow">
<pre>
@inproceedings{HossainPLJBR12,
  author = {K. S. M. Tozammel Hossain and Debprakash Patnaik and Srivatsan Laxman and Prateek Jain and Chris Bailey-Kellogg and Naren Ramakrishnan},
  title = {Improved multiple sequence alignments using coupled pattern mining},
  booktitle = {Proceedings of the ACM International Conference on Bioinformatics, Computational Biology and Biomedicine (BCB)},
  year = {2012},
  pages = {28--35},
  url = {http://doi.acm.org/10.1145/2382936.2382940},
  doi = {https://doi.org/10.1145/2382936.2382940}
}
</pre>
</div>
</li>
<br /><!-- Item: JainT12 -->
<li >
<b>Mirror Descent Based Database Privacy</b><br>
Prateek Jain and Abhradeep Thakurta,<br>
in <i>Proceedings of the 16th International Workshop (RANDOM)</i>,
2012.
<br />
<a href="javascript:toggleBibtex('JainT12')">[BibTeX]</a>
<a id="displayTextJainT12" href="javascript:toggle('JainT12');">[Abstract]</a>
<a href="http://dx.doi.org/10.1007/978-3-642-32512-0_49">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainT12" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee">In this paper, we focus on the problem of private database
release in the interactive setting: a trusted database curator receives
queries in an online manner for which it needs to respond with accurate
but privacy preserving answers. To this end, we generalize the IDC (Iter-
ative Database Construction) framework of [HR10,GRU11] that maintains a dif-
ferentially private artificial dataset and answers incoming linear queries
using the artificial dataset. In particular, we formulate a generic IDC
framework based on the Mirror Descent algorithm, a popular convex
optimization algorithm. We then present two concrete applications,
namely, cut queries over a bipartite graph and linear queries over low-
rank matrices, and provide significantly tighter error bounds than the
ones by [HR10, GRU11].</div>
<div id="bib_JainT12" class="bibtex noshow">
<pre>
@inproceedings{JainT12,
  author = {Prateek Jain and Abhradeep Thakurta},
  title = {Mirror Descent Based Database Privacy},
  booktitle = {Proceedings of the 16th International Workshop (RANDOM)},
  year = {2012},
  pages = {579--590},
  url = {http://dx.doi.org/10.1007/978-3-642-32512-0_49},
  doi = {https://doi.org/10.1007/978-3-642-32512-0_49}
}
</pre>
</div>
</li>
<br /><!-- Item: JainKT12 -->
<li >
<b>Differentially Private Online Learning</b><br>
Prateek Jain, Pravesh Kothari and Abhradeep Thakurta,<br>
in <i>Proceedings of the 25th Annual Conference on Learning Theory (COLT)</i>,
2012.
<br />
<a href="javascript:toggleBibtex('JainKT12')">[BibTeX]</a>
<a id="displayTextJainKT12" href="javascript:toggle('JainKT12');">[Abstract]</a>
<a href="http://www.jmlr.org/proceedings/papers/v23/jain12/jain12.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainKT12" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainKT12" class="bibtex noshow">
<pre>
@inproceedings{JainKT12,
  author = {Prateek Jain and Pravesh Kothari and Abhradeep Thakurta},
  title = {Differentially Private Online Learning},
  booktitle = {Proceedings of the 25th Annual Conference on Learning Theory (COLT)},
  year = {2012},
  pages = {24.1--24.34},
  url = {http://www.jmlr.org/proceedings/papers/v23/jain12/jain12.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JainTD11 -->
<li >
<b>Orthogonal Matching Pursuit with Replacement</b><br>
Prateek Jain, Ambuj Tewari and Inderjit S. Dhillon,<br>
in <i>Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2011.
<br />
<a href="javascript:toggleBibtex('JainTD11')">[BibTeX]</a>
<a id="displayTextJainTD11" href="javascript:toggle('JainTD11');">[Abstract]</a>
<a href="http://books.nips.cc/papers/files/nips24/NIPS2011_0707.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainTD11" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainTD11" class="bibtex noshow">
<pre>
@inproceedings{JainTD11,
  author = {Prateek Jain and Ambuj Tewari and Inderjit S. Dhillon},
  title = {Orthogonal Matching Pursuit with Replacement},
  booktitle = {Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2011},
  pages = {1215--1223},
  url = {http://books.nips.cc/papers/files/nips24/NIPS2011_0707.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: KarJ11 -->
<li >
<b>Similarity-based Learning via Data Driven Embeddings</b><br>
Purushottam Kar and Prateek Jain,<br>
in <i>Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2011.
<br />
<a href="javascript:toggleBibtex('KarJ11')">[BibTeX]</a>
<a id="displayTextKarJ11" href="javascript:toggle('KarJ11');">[Abstract]</a>
<a href="http://books.nips.cc/papers/files/nips24/NIPS2011_1129.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextKarJ11" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_KarJ11" class="bibtex noshow">
<pre>
@inproceedings{KarJ11,
  author = {Purushottam Kar and Prateek Jain},
  title = {Similarity-based Learning via Data Driven Embeddings},
  booktitle = {Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2011},
  pages = {1998--2006},
  url = {http://books.nips.cc/papers/files/nips24/NIPS2011_1129.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JainVG10 -->
<li >
<b>Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</b><br>
Prateek Jain, Sudheendra Vijayanarasimhan and Kristen Grauman,<br>
in <i>Proceedings of the 24th Annual Conference on Neural Information Processing Systems 2010 (NIPS)</i>,
2010.
<br />
<a href="javascript:toggleBibtex('JainVG10')">[BibTeX]</a>
<a id="displayTextJainVG10" href="javascript:toggle('JainVG10');">[Abstract]</a>
<a href="http://books.nips.cc/papers/files/nips23/NIPS2010_0757.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainVG10" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainVG10" class="bibtex noshow">
<pre>
@inproceedings{JainVG10,
  author = {Prateek Jain and Sudheendra Vijayanarasimhan and Kristen Grauman},
  title = {Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning},
  booktitle = {Proceedings of the 24th Annual Conference on Neural Information Processing Systems 2010 (NIPS)},
  year = {2010},
  pages = {928--936},
  url = {http://books.nips.cc/papers/files/nips23/NIPS2010_0757.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JainKD10 -->
<li >
<b>Inductive Regularized Learning of Kernel Functions</b><br>
Prateek Jain, Brian Kulis and Inderjit S. Dhillon,<br>
in <i>Proceedings of the 24th Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2010.
<br />
<a href="javascript:toggleBibtex('JainKD10')">[BibTeX]</a>
<a id="displayTextJainKD10" href="javascript:toggle('JainKD10');">[Abstract]</a>
<a href="http://books.nips.cc/papers/files/nips23/NIPS2010_0603.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainKD10" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainKD10" class="bibtex noshow">
<pre>
@inproceedings{JainKD10,
  author = {Prateek Jain and Brian Kulis and Inderjit S. Dhillon},
  title = {Inductive Regularized Learning of Kernel Functions},
  booktitle = {Proceedings of the 24th Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2010},
  pages = {946--954},
  url = {http://books.nips.cc/papers/files/nips23/NIPS2010_0603.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: JainMD10 -->
<li >
<b>Guaranteed Rank Minimization via Singular Value Projection</b><br>
Prateek Jain, Raghu Meka and Inderjit S. Dhillon,<br>
in <i>Proceedings of the 24th Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2010.
<br />
<a href="javascript:toggleBibtex('JainMD10')">[BibTeX]</a>
<a id="displayTextJainMD10" href="javascript:toggle('JainMD10');">[Abstract]</a>
<a href="http://books.nips.cc/papers/files/nips23/NIPS2010_0682.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainMD10" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainMD10" class="bibtex noshow">
<pre>
@inproceedings{JainMD10,
  author = {Prateek Jain and Raghu Meka and Inderjit S. Dhillon},
  title = {Guaranteed Rank Minimization via Singular Value Projection},
  booktitle = {Proceedings of the 24th Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2010},
  pages = {937--945},
  url = {http://books.nips.cc/papers/files/nips23/NIPS2010_0682.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: VijayanarasimhanJG10 -->
<li >
<b>Far-sighted active learning on a budget for image and video recognition</b><br>
Sudheendra Vijayanarasimhan, Prateek Jain and Kristen Grauman,<br>
in <i>Proceedings of the Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>,
2010.
<br />
<a href="javascript:toggleBibtex('VijayanarasimhanJG10')">[BibTeX]</a>
<a id="displayTextVijayanarasimhanJG10" href="javascript:toggle('VijayanarasimhanJG10');">[Abstract]</a>
<a href="http://dx.doi.org/10.1109/CVPR.2010.5540055">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextVijayanarasimhanJG10" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_VijayanarasimhanJG10" class="bibtex noshow">
<pre>
@inproceedings{VijayanarasimhanJG10,
  author = {Sudheendra Vijayanarasimhan and Prateek Jain and Kristen Grauman},
  title = {Far-sighted active learning on a budget for image and video recognition},
  booktitle = {Proceedings of the Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2010},
  pages = {3035--3042},
  url = {http://dx.doi.org/10.1109/CVPR.2010.5540055},
  doi = {https://doi.org/10.1109/CVPR.2010.5540055}
}
</pre>
</div>
</li>
<br /><!-- Item: MekaJD09 -->
<li >
<b>Matrix Completion from Power-Law Distributed Samples</b><br>
Raghu Meka, Prateek Jain and Inderjit S. Dhillon,<br>
in <i>Proceedings of the 23rd Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2009.
<br />
<a href="javascript:toggleBibtex('MekaJD09')">[BibTeX]</a>
<a id="displayTextMekaJD09" href="javascript:toggle('MekaJD09');">[Abstract]</a>
<a href="http://books.nips.cc/papers/files/nips22/NIPS2009_0864.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextMekaJD09" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_MekaJD09" class="bibtex noshow">
<pre>
@inproceedings{MekaJD09,
  author = {Raghu Meka and Prateek Jain and Inderjit S. Dhillon},
  title = {Matrix Completion from Power-Law Distributed Samples},
  booktitle = {Proceedings of the 23rd Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2009},
  pages = {1258--1266},
  url = {http://books.nips.cc/papers/files/nips22/NIPS2009_0864.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: LuJD09 -->
<li >
<b>Geometry-aware metric learning</b><br>
Zhengdong Lu, Prateek Jain and Inderjit S. Dhillon,<br>
in <i>Proceedings of the 26th Annual International Conference on Machine Learning (ICML)</i>,
2009.
<br />
<a href="javascript:toggleBibtex('LuJD09')">[BibTeX]</a>
<a id="displayTextLuJD09" href="javascript:toggle('LuJD09');">[Abstract]</a>
<a href="http://doi.acm.org/10.1145/1553374.1553461">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextLuJD09" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_LuJD09" class="bibtex noshow">
<pre>
@inproceedings{LuJD09,
  author = {Zhengdong Lu and Prateek Jain and Inderjit S. Dhillon},
  title = {Geometry-aware metric learning},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning (ICML)},
  year = {2009},
  pages = {673--680},
  url = {http://doi.acm.org/10.1145/1553374.1553461},
  doi = {https://doi.org/10.1145/1553374.1553461}
}
</pre>
</div>
</li>
<br /><!-- Item: JainK09 -->
<li >
<b>Active learning for large multi-class problems</b><br>
Prateek Jain and Ashish Kapoor,<br>
in <i>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</i>,
2009.
<br />
<a href="javascript:toggleBibtex('JainK09')">[BibTeX]</a>
<a id="displayTextJainK09" href="javascript:toggle('JainK09');">[Abstract]</a>
<a href="http://dx.doi.org/10.1109/CVPRW.2009.5206651">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainK09" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainK09" class="bibtex noshow">
<pre>
@inproceedings{JainK09,
  author = {Prateek Jain and Ashish Kapoor},
  title = {Active learning for large multi-class problems},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2009},
  pages = {762--769},
  url = {http://dx.doi.org/10.1109/CVPRW.2009.5206651},
  doi = {https://doi.org/10.1109/CVPRW.2009.5206651}
}
</pre>
</div>
</li>
<br /><!-- Item: JainKDG08 -->
<li >
<b>Online Metric Learning and Fast Similarity Search</b><br>
Prateek Jain, Brian Kulis, Inderjit S. Dhillon and Kristen Grauman,<br>
in <i>Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems (NIPS)</i>,
2008.
<br />
<a href="javascript:toggleBibtex('JainKDG08')">[BibTeX]</a>
<a id="displayTextJainKDG08" href="javascript:toggle('JainKDG08');">[Abstract]</a>
<a href="http://books.nips.cc/papers/files/nips21/NIPS2008_1003.pdf">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainKDG08" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainKDG08" class="bibtex noshow">
<pre>
@inproceedings{JainKDG08,
  author = {Prateek Jain and Brian Kulis and Inderjit S. Dhillon and Kristen Grauman},
  title = {Online Metric Learning and Fast Similarity Search},
  booktitle = {Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2008},
  pages = {761--768},
  url = {http://books.nips.cc/papers/files/nips21/NIPS2008_1003.pdf}
}
</pre>
</div>
</li>
<br /><!-- Item: MekaJCD08 -->
<li >
<b>Rank minimization via online learning</b><br>
Raghu Meka, Prateek Jain, Constantine Caramanis and Inderjit S. Dhillon,<br>
in <i>Proceedings of the Twenty-Fifth International Conference on Machine Learning (ICML)</i>,
2008.
<br />
<a href="javascript:toggleBibtex('MekaJCD08')">[BibTeX]</a>
<a id="displayTextMekaJCD08" href="javascript:toggle('MekaJCD08');">[Abstract]</a>
<a href="http://doi.acm.org/10.1145/1390156.1390239">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextMekaJCD08" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_MekaJCD08" class="bibtex noshow">
<pre>
@inproceedings{MekaJCD08,
  author = {Raghu Meka and Prateek Jain and Constantine Caramanis and Inderjit S. Dhillon},
  title = {Rank minimization via online learning},
  booktitle = {Proceedings of the Twenty-Fifth International Conference on Machine Learning (ICML)},
  year = {2008},
  pages = {656--663},
  url = {http://doi.acm.org/10.1145/1390156.1390239},
  doi = {https://doi.org/10.1145/1390156.1390239}
}
</pre>
</div>
</li>
<br /><!-- Item: JainKG08 -->
<li >
<b>Fast image search for learned metrics</b><br>
Prateek Jain, Brian Kulis and Kristen Grauman,<br>
in <i>Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</i>,
2008.
<br />
<a href="javascript:toggleBibtex('JainKG08')">[BibTeX]</a>
<a id="displayTextJainKG08" href="javascript:toggle('JainKG08');">[Abstract]</a>
<a href="http://dx.doi.org/10.1109/CVPR.2008.4587841">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainKG08" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainKG08" class="bibtex noshow">
<pre>
@inproceedings{JainKG08,
  author = {Prateek Jain and Brian Kulis and Kristen Grauman},
  title = {Fast image search for learned metrics},
  booktitle = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2008},
  url = {http://dx.doi.org/10.1109/CVPR.2008.4587841},
  doi = {https://doi.org/10.1109/CVPR.2008.4587841}
}
</pre>
</div>
</li>
<br /><!-- Item: JainMD08 -->
<li >
<b>Simultaneous Unsupervised Learning of Disparate Clusterings</b><br>
Prateek Jain, Raghu Meka and Inderjit S. Dhillon,<br>
in <i>Proceedings of the SIAM International Conference on Data Mining (SDM)</i>,
2008.
<br />
<a href="javascript:toggleBibtex('JainMD08')">[BibTeX]</a>
<a id="displayTextJainMD08" href="javascript:toggle('JainMD08');">[Abstract]</a>
<a href="http://dx.doi.org/10.1137/1.9781611972788.77">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextJainMD08" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_JainMD08" class="bibtex noshow">
<pre>
@inproceedings{JainMD08,
  author = {Prateek Jain and Raghu Meka and Inderjit S. Dhillon},
  title = {Simultaneous Unsupervised Learning of Disparate Clusterings},
  booktitle = {Proceedings of the SIAM International Conference on Data Mining (SDM)},
  year = {2008},
  pages = {858--869},
  url = {http://dx.doi.org/10.1137/1.9781611972788.77},
  doi = {https://doi.org/10.1137/1.9781611972788.77}
}
</pre>
</div>
</li>
<br /><!-- Item: DavisKJSD07 -->
<li >
<b>Information-theoretic metric learning</b><br>
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra and Inderjit S. Dhillon,<br>
in <i>Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML)</i>,
2007.
<br />
<a href="javascript:toggleBibtex('DavisKJSD07')">[BibTeX]</a>
<a id="displayTextDavisKJSD07" href="javascript:toggle('DavisKJSD07');">[Abstract]</a>
<a href="http://doi.acm.org/10.1145/1273496.1273523">[URL]</a>

<script type="text/javascript">
var slide="";
if(slide==""){
document.write("");
}else{
document.write("<a href=>[Slides]</a>");
}
var poster="";
var code="";
if(poster==""){
document.write("");
}else{
document.write("<a href=>[Poster]</a>");
}
if(code==""){
document.write("");
}else{
document.write("<a href=>[Code]</a>");
}
</script>



<div id="toggleTextDavisKJSD07" style="display: none;font-size: 90%;margin-right: 0%;margin-top: 1.2em;margin-bottom: 1.3em;border: 1px solid silver;padding: 0.3em 0.5em;background: #ffffee"></div>
<div id="bib_DavisKJSD07" class="bibtex noshow">
<pre>
@inproceedings{DavisKJSD07,
  author = {Jason V. Davis and Brian Kulis and Prateek Jain and Suvrit Sra and Inderjit S. Dhillon},
  title = {Information-theoretic metric learning},
  booktitle = {Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML)},
  year = {2007},
  pages = {209--216},
  url = {http://doi.acm.org/10.1145/1273496.1273523},
  doi = {https://doi.org/10.1145/1273496.1273523}
}
</pre>
</div>
</li>
<br />